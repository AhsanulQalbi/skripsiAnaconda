{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKRIPSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, validation_curve,RandomizedSearchCV\n",
    "from matplotlib_venn import venn2\n",
    "from time import time\n",
    "\n",
    "np.random.seed(0)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "poster_Stemmer = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Amazon_Train.csv' does not exist: b'Amazon_Train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3782b2d09c25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#baca csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_train_amazon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Amazon_Train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata_train_yelp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Yelp_Train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_test_amazon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Amazon_Test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_test_yelp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Yelp_Test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Amazon_Train.csv' does not exist: b'Amazon_Train.csv'"
     ]
    }
   ],
   "source": [
    "#baca csv\n",
    "data_train_amazon = pd.read_csv('Amazon_Train.csv')\n",
    "data_train_yelp = pd.read_csv('Yelp_Train.csv')\n",
    "data_test_amazon = pd.read_csv('Amazon_Test.csv')\n",
    "data_test_yelp = pd.read_csv('Yelp_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing ke lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_Preprocessing_Amazon = data_amazon\n",
    "data_Lowercase_Amazon = []\n",
    "\n",
    "data_Preprocessing_Yelp = data_yelp\n",
    "data_Lowercase_Yelp = []\n",
    "\n",
    "while iterator < len(data_amazon) :\n",
    "    data_Lowercase_Amazon.append(data_amazon.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_yelp) :\n",
    "    data_Lowercase_Yelp.append(data_yelp.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon['Lowercase'] = data_Lowercase_Amazon\n",
    "data_Preprocessing_Amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp['Lowercase'] = data_Lowercase_Yelp\n",
    "data_Preprocessing_Yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemoveNumber_Amazon = []\n",
    "\n",
    "data_RemoveNumber_Yelp = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon) :\n",
    "    data_RemoveNumber_Amazon.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Amazon.Lowercase[iterator]))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp) :\n",
    "    data_RemoveNumber_Yelp.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Yelp.Lowercase[iterator]))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon['RemoveNumber'] = data_RemoveNumber_Amazon\n",
    "data_Preprocessing_Amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp['RemoveNumber'] = data_RemoveNumber_Yelp\n",
    "data_Preprocessing_Yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan tanda baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemovePunctuation_Amazon = []\n",
    "\n",
    "data_RemovePunctuation_Yelp = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon) :\n",
    "    data_RemovePunctuation_Amazon.append(data_Preprocessing_Amazon.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp) :\n",
    "    data_RemovePunctuation_Yelp.append(data_Preprocessing_Yelp.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon['RemovePunctuation'] = data_RemovePunctuation_Amazon\n",
    "data_Preprocessing_Amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp['RemovePunctuation'] = data_RemovePunctuation_Yelp\n",
    "data_Preprocessing_Yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan Non alfabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "\n",
    "data_Regex_alpabet_only_Amazon = []\n",
    "\n",
    "data_Regex_alpabet_only_Yelp = []\n",
    "\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon) :\n",
    "    data_Regex_alpabet_only_Amazon.append(\" \".join(re.findall(\"[a-zA-Z]+\", data_Preprocessing_Amazon.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp) :\n",
    "    data_Regex_alpabet_only_Yelp.append(\" \".join(re.findall(r\"[a-zA-Z]+\", data_Preprocessing_Yelp.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon['Regex'] = data_Regex_alpabet_only_Amazon\n",
    "data_Preprocessing_Amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp['Regex'] = data_Regex_alpabet_only_Yelp\n",
    "data_Preprocessing_Yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Amazon_Sentiment = data_Preprocessing_Amazon['Regex']\n",
    "Clean_Yelp_Sentiment = data_Preprocessing_Yelp['Regex']\n",
    "Label_Amazon = data_Preprocessing_Amazon['Label']\n",
    "Label_Yelp = data_Preprocessing_Yelp['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram (bigram dan trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "Vocabulary_Amazon = []\n",
    "ngram_amazon = []\n",
    "temporary = []\n",
    "\n",
    "while iterator < len(Clean_Amazon_Sentiment) :\n",
    "    ngram = 2\n",
    "    sentence = Clean_Amazon_Sentiment[iterator]\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = vocab\n",
    "\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Amazon) :\n",
    "            Vocabulary_Amazon.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "    \n",
    "    ngram = 3\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = temp + vocab\n",
    "    ngram_amazon.append(temp)\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Amazon) :\n",
    "            Vocabulary_Amazon.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "        \n",
    "    iterator = iterator + 1\n",
    "    vocab = None\n",
    "    del vocab\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "Vocabulary_Yelp = []\n",
    "ngram_yelp = []\n",
    "temporary = []\n",
    "\n",
    "while iterator < len(Clean_Yelp_Sentiment) :\n",
    "    ngram = 2\n",
    "    sentence = Clean_Yelp_Sentiment[iterator]\n",
    "    \n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "  \n",
    "    temp = vocab\n",
    "\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Yelp) :\n",
    "            Vocabulary_Yelp.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "    \n",
    "    ngram = 3\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = temp + vocab\n",
    "    ngram_yelp.append(temp)\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Yelp) :\n",
    "            Vocabulary_Yelp.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "        \n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me inisialisasi jumlah frekuensi Gram di Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Amazon = []\n",
    "WordDictionaryCount_Yelp = []\n",
    "\n",
    "iterator = 0\n",
    "while iterator < len(Clean_Amazon_Sentiment):\n",
    "    WordDictionaryCount_Amazon.append(dict.fromkeys(Vocabulary_Amazon, 0))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(Clean_Yelp_Sentiment):\n",
    "    WordDictionaryCount_Yelp.append(dict.fromkeys(Vocabulary_Yelp, 0))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "while iterator < len(Clean_Amazon_Sentiment) :\n",
    "    for gram in ngram_amazon[iterator] :\n",
    "        WordDictionaryCount_Amazon[iterator][gram] += 1\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(Clean_Yelp_Sentiment) :\n",
    "    for gram in ngram_yelp[iterator] :\n",
    "        WordDictionaryCount_Yelp[iterator][gram] += 1\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Amazon[0],WordDictionaryCount_Amazon[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Yelp[0],WordDictionaryCount_Yelp[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai TF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, ngram):\n",
    "    tfDict = {}\n",
    "    ngramLength = len(ngram)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(ngramLength)\n",
    "    return tfDict\n",
    "\n",
    "Tf_Sentimen_Amazon = []\n",
    "Tf_Sentimen_Yelp = []\n",
    "\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Amazon):\n",
    "    Tf_Sentimen_Amazon.append(computeTF(WordDictionaryCount_Amazon[iterator], ngram_amazon[iterator]))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp):\n",
    "    Tf_Sentimen_Yelp.append(computeTF(WordDictionaryCount_Yelp[iterator], ngram_yelp[iterator]))\n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai IDF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def computeIDF(WordDict):\n",
    "    idfDict = {}\n",
    "    N = len(WordDict)\n",
    "    \n",
    "    idfDict = dict.fromkeys(WordDict[0].keys(), 0)\n",
    "    for doc in WordDict:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "IDF_Amazon = computeIDF(WordDictionaryCount_Amazon)\n",
    "IDF_Yelp = computeIDF(WordDictionaryCount_Yelp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai TF-IDF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(Tf_Sentimen_Amazon, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in Tf_Sentimen_Amazon.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "TF_IDF_Amazon = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Amazon):\n",
    "    TF_IDF_Amazon.append(computeTFIDF(Tf_Sentimen_Amazon[iterator], IDF_Amazon))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "TF_IDF_Yelp = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp):\n",
    "    TF_IDF_Yelp.append(computeTFIDF(Tf_Sentimen_Yelp[iterator], IDF_Yelp))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menampung nilai TF-IDF pada variabel Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Amazon = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Amazon):\n",
    "    Vectorizer_Amazon.append(list(TF_IDF_Amazon[iterator].values()))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "Vectorizer_Yelp = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Yelp):\n",
    "    Vectorizer_Yelp.append(list(TF_IDF_Yelp[iterator].values()))\n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame Vectorizer Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Amazon = pd.DataFrame(Vectorizer_Amazon, columns = Vocabulary_Amazon)\n",
    "print (DataFrame_Amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Vectorizer Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Yelp = pd.DataFrame(Vectorizer_Yelp, columns = Vocabulary_Yelp)\n",
    "print (DataFrame_Yelp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jumlah data label positif dan negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data positif negatif amazon\")\n",
    "DataFrame_Label = pd.DataFrame(Label_Amazon)\n",
    "print(DataFrame_Label['Label'].value_counts())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Data positif negatif yelp\")\n",
    "DataFrame_Label = pd.DataFrame(Label_Yelp)\n",
    "print(DataFrame_Label['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split data amazon dan yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_amazon, data_test_amazon, label_train_amazon, label_test_amazon = train_test_split((Vectorizer_Amazon), Label_Amazon, random_state = 0)\n",
    "data_train_yelp, data_test_yelp, label_train_yelp, label_test_yelp = train_test_split((Vectorizer_Yelp), Label_Yelp, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amazon data\")\n",
    "data_train_amazon = np.array(data_train_amazon)\n",
    "data_train_yelp = np.array(data_train_yelp)\n",
    "data_test_amazon = np.array(data_test_amazon)\n",
    "data_test_yelp = np.array(data_test_yelp)\n",
    "\n",
    "label_train_amazon = np.array(label_train_amazon)\n",
    "label_train_yelp = np.array(label_train_yelp)\n",
    "label_test_amazon = np.array(label_test_amazon)\n",
    "label_test_yelp = np.array(label_test_yelp)\n",
    "\n",
    "print(\"data train shape : {}\".format(data_train_amazon.shape))\n",
    "print(\"data test shape : {}\".format(data_test_amazon.shape))\n",
    "print(\"label train shape : {}\".format(label_train_amazon.shape))\n",
    "print(\"label test shape : {}\".format(label_test_amazon.shape))\n",
    "\n",
    "print(\"\\nYelp data\")\n",
    "print(\"data train shape : {}\".format(data_train_yelp.shape))\n",
    "print(\"data test shape : {}\".format(data_test_yelp.shape))\n",
    "print(\"label train shape : {}\".format(label_train_yelp.shape))\n",
    "print(\"label test shape : {}\".format(label_test_yelp.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# dr = PCA(n_components = 300)\n",
    "# dr.fit(data_train_amazon)\n",
    "# data_train_amazon = dr.transform(data_train_amazon)\n",
    "# data_test_amazon = dr.transform(data_test_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skor Akurasi Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_Classifier_Amazon = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Amazon.fit(data_train_amazon, label_train_amazon)\n",
    "print(\"skor RF_Amazon: {}\".format(RF_Classifier_Amazon.score(data_test_amazon, label_test_amazon)))\n",
    "\n",
    "RF_Classifier_Yelp = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Yelp.fit(data_train_yelp, label_train_yelp)\n",
    "print(\"skor RF_Yelp: {}\".format(RF_Classifier_Yelp.score(data_test_yelp, label_test_yelp)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mencari Fitur Importance > 0 dari Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Importance_Amazon = []\n",
    "iterator = 0\n",
    "length = len (Vocabulary_Amazon)\n",
    "\n",
    "while iterator < length : \n",
    "    if RF_Classifier_Amazon.feature_importances_[iterator] > 0 :\n",
    "        Vocabulary_Importance_Amazon.append(Vocabulary_Amazon[iterator])\n",
    "       \n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengetahui Akurasi jika diterapkan Transfer learning Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Jika diterapkan data interseksi antara Vocabulary_Amazon (fi > 0) dan Vocabulary Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Intersection = list(set(Vocabulary_Importance_Amazon) & set (Vocabulary_Yelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WordDictionaryCount_Intersection = []\n",
    "\n",
    "iterator = 0\n",
    "while iterator < len(ngram_yelp):\n",
    "    WordDictionaryCount_Intersection.append(dict.fromkeys(Vocabulary_Intersection, 0))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "while iterator < len(ngram_yelp) :\n",
    "    for gram in ngram_yelp[iterator] :\n",
    "        if gram in Vocabulary_Intersection :\n",
    "            WordDictionaryCount_Intersection[iterator][gram] += 1\n",
    "    iterator = iterator + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "TF_Intersection = []\n",
    "while iterator < len(WordDictionaryCount_Intersection):\n",
    "    TF_Intersection.append(computeTF(WordDictionaryCount_Intersection[iterator], ngram_yelp[iterator]))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "IDF_Intersection = computeIDF(WordDictionaryCount_Intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_Intersection = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Intersection):\n",
    "    TF_IDF_Intersection.append(computeTFIDF(TF_Intersection[iterator], IDF_Amazon))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Intersection = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Intersection):\n",
    "    Vectorizer_Intersection.append(list(TF_IDF_Intersection[iterator].values()))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Intersection = pd.DataFrame(Vectorizer_Intersection, columns = Vocabulary_Intersection)\n",
    "print (DataFrame_Intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_intersec, data_test_intersec, label_train_intersec, label_test_intersec = train_test_split((Vectorizer_Intersection), Label_Yelp, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akurasi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Classifier_Intersec = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Intersec.fit(data_train_intersec, label_train_intersec)\n",
    "print(\"skor Intersec_Model: {}\".format(RF_Classifier_Intersec.score(data_test_intersec, label_test_intersec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jika diterapkan data gabungan antara Vocabulary_Amazon (fi > 0) dan Vocabulary Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Gabungan = []\n",
    "iterator = 0\n",
    "length = len(Vocabulary_Yelp)\n",
    "\n",
    "while iterator < length : \n",
    "#     jika interseksi dengan amazon dan bukan fitur importance maka vocab tidak dimasukan\n",
    "    if  (Vocabulary_Yelp[iterator] in Vocabulary_Amazon) and (Vocabulary_Yelp[iterator] not in Vocabulary_Importance_Amazon) : \n",
    "        iterator = iterator + 1\n",
    "        continue\n",
    "        \n",
    "    Vocabulary_Gabungan.append(Vocabulary_Yelp[iterator])\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Gabungan = []\n",
    "\n",
    "iterator = 0\n",
    "while iterator < len(ngram_yelp):\n",
    "    WordDictionaryCount_Gabungan.append(dict.fromkeys(Vocabulary_Gabungan, 0))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "while iterator < len(ngram_yelp) :\n",
    "    for gram in ngram_yelp[iterator] :\n",
    "        if gram in Vocabulary_Gabungan :\n",
    "            WordDictionaryCount_Gabungan[iterator][gram] += 1\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "TF_Gabungan = []\n",
    "while iterator < len(WordDictionaryCount_Gabungan):\n",
    "    TF_Gabungan.append(computeTF(WordDictionaryCount_Gabungan[iterator], ngram_yelp[iterator]))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "IDF_Gabungan = computeIDF(WordDictionaryCount_Gabungan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_Gabungan = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Gabungan):\n",
    "    TF_IDF_Gabungan.append(computeTFIDF(TF_Gabungan[iterator], IDF_Gabungan))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Gabungan = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Gabungan):\n",
    "    Vectorizer_Gabungan.append(list(TF_IDF_Gabungan[iterator].values()))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Gabungan = pd.DataFrame(Vectorizer_Gabungan, columns = Vocabulary_Gabungan)\n",
    "print(DataFrame_Gabungan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_gabungan, data_test_gabungan, label_train_gabungan, label_test_gabungan = train_test_split((Vectorizer_Gabungan), Label_Yelp, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Classifier_Gabungan = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Gabungan.fit(data_train_gabungan, label_train_gabungan)\n",
    "print(\"skor Model: {}\".format(RF_Classifier_Gabungan.score(data_test_gabungan, label_test_gabungan)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
