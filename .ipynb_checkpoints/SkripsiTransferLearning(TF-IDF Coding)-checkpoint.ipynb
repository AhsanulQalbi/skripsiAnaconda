{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKRIPSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, validation_curve,RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from matplotlib_venn import venn2\n",
    "from time import time\n",
    "\n",
    "np.random.seed(0)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "poster_Stemmer = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baca csv\n",
    "data_train_amazon = pd.read_csv('Amazon_Train.csv')\n",
    "data_train_yelp = pd.read_csv('Yelp_Train.csv')\n",
    "data_test_amazon = pd.read_csv('Amazon_Test.csv')\n",
    "data_test_yelp = pd.read_csv('Yelp_Test.csv')\n",
    "data_train_imdb = pd.read_csv('IMDB_Train.csv')\n",
    "data_test_imdb = pd.read_csv('IMDB_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label                                           Sentimen\n",
      "0      0  Buyer beware This is a self-published book, an...\n",
      "1      0  The Worst! A complete waste of time. Typograph...\n",
      "2      0  Oh please I guess you have to be a romance nov...\n",
      "3      0  Awful beyond belief! I feel I have to write to...\n",
      "4      0  Another Abysmal Digital Copy Rather than scrat...\n",
      "\n",
      "\n",
      "   Label                                           Sentimen\n",
      "0      0  I don't know what Dr. Goldberg was like before...\n",
      "1      0  I'm writing this review to give you a heads up...\n",
      "2      0  Owning a driving range inside the city limits ...\n",
      "3      0  This place is absolute garbage...  Half of the...\n",
      "4      0  Used to go there for tires, brakes, etc.  Thei...\n",
      "\n",
      "\n",
      "   Label                                           Sentimen\n",
      "0      0  Story of a man who has unnatural feelings for ...\n",
      "1      0  Airport '77 starts as a brand new luxury 747 p...\n",
      "2      0  This film lacked something I couldn't put my f...\n",
      "3      0  Sorry everyone,,, I know this is supposed to b...\n",
      "4      0  When I was little my parents took me along to ...\n"
     ]
    }
   ],
   "source": [
    "print(data_train_amazon.head())\n",
    "print(\"\\n\")\n",
    "print(data_train_yelp.head())\n",
    "print(\"\\n\")\n",
    "print(data_train_imdb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label                                           Sentimen\n",
      "0      0  Overly complicated Being both a U.S. history b...\n",
      "1      0  Terrible Disappointment -- parts don't fit My ...\n",
      "2      0  Didn't hold up.....very disappointed I bought ...\n",
      "3      0  gene hates jezebel i love JLJ but this compila...\n",
      "4      0  Nice toy but ... My six-year-old loves space a...\n",
      "\n",
      "\n",
      "   Label                                           Sentimen\n",
      "0      0  My wife and I used to love Arriba's, til recen...\n",
      "1      0  You get what you pay for.  The food is inexpen...\n",
      "2      0  Unfortunately, yesterday's visit was one of th...\n",
      "3      0  I went into the Scottsdale location yesterday....\n",
      "4      0  It takes a lot for me to write a review and bl...\n"
     ]
    }
   ],
   "source": [
    "print(data_test_amazon.head())\n",
    "print(\"\\n\")\n",
    "print(data_test_yelp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label                                           Sentimen\n",
      "0      0  Story of a man who has unnatural feelings for ...\n",
      "1      0  Airport '77 starts as a brand new luxury 747 p...\n",
      "2      0  This film lacked something I couldn't put my f...\n",
      "3      0  Sorry everyone,,, I know this is supposed to b...\n",
      "4      0  When I was little my parents took me along to ...\n",
      "\n",
      "\n",
      "   Label                                           Sentimen\n",
      "0      0  Once again Mr. Costner has dragged out a movie...\n",
      "1      0  First of all, I would like to say that I am a ...\n",
      "2      0  I'm a huge fan of both Emily Watson (Breaking ...\n",
      "3      0  I was pulled into this movie early on, much to...\n",
      "4      0  This tale of the upper-classes getting their c...\n"
     ]
    }
   ],
   "source": [
    "print(data_train_imdb.head())\n",
    "print(\"\\n\")\n",
    "print(data_test_imdb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train = data_train_amazon.Sentimen.astype(str)\n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(lambda x: x.lower())\n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(\n",
    "                                    lambda x: x.translate(str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(\n",
    "                                    lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
    "\n",
    "data_Preprocessing_Yelp_Train = data_train_yelp.Sentimen.astype(str)\n",
    "data_Preprocessing_Yelp_Train = data_Preprocessing_Yelp_Train.apply(lambda x: x.lower())\n",
    "data_Preprocessing_Yelp_Train = data_Preprocessing_Yelp_Train.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_Yelp_Train = data_Preprocessing_Yelp_Train.apply(\n",
    "                                    lambda x: x.translate(str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_Yelp_Train = data_Preprocessing_Yelp_Train.apply(\n",
    "                                    lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
    "\n",
    "data_Preprocessing_IMDB_Train = data_train_imdb.Sentimen.astype(str)\n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(lambda x: x.lower())\n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(\n",
    "                                    lambda x: x.translate(str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(\n",
    "                                    lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    buyer beware this is a selfpublished book and ...\n",
       "1    the worst a complete waste of time typographic...\n",
       "2    oh please i guess you have to be a romance nov...\n",
       "3    awful beyond belief i feel i have to write to ...\n",
       "4    another abysmal digital copy rather than scrat...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i dont know what dr goldberg was like before m...\n",
       "1    im writing this review to give you a heads up ...\n",
       "2    owning a driving range inside the city limits ...\n",
       "3    this place is absolute garbage half of the tee...\n",
       "4    used to go there for tires brakes etc their pr...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    story of a man who has unnatural feelings for ...\n",
       "1    airport starts as a brand new luxury plane is ...\n",
       "2    this film lacked something i couldnt put my fi...\n",
       "3    sorry everyone i know this is supposed to be a...\n",
       "4    when i was little my parents took me along to ...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_IMDB_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test = data_test_amazon.Sentimen.astype(str)\n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: x.lower())\n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: x.translate(\n",
    "                                str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: \" \".join(\n",
    "                                re.findall(\"[a-zA-Z]+\", x)))\n",
    "\n",
    "data_Preprocessing_Yelp_Test = data_test_yelp.Sentimen.astype(str)\n",
    "data_Preprocessing_Yelp_Test = data_Preprocessing_Yelp_Test.apply(lambda x: x.lower())\n",
    "data_Preprocessing_Yelp_Test = data_Preprocessing_Yelp_Test.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_Yelp_Test = data_Preprocessing_Yelp_Test.apply(lambda x: x.translate(\n",
    "                                str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_Yelp_Test = data_Preprocessing_Yelp_Test.apply(lambda x: \" \".join(\n",
    "                                re.findall(\"[a-zA-Z]+\", x)))\n",
    "\n",
    "data_Preprocessing_IMDB_Test = data_test_imdb.Sentimen.astype(str)\n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(lambda x: x.lower())\n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(lambda x: x.translate(\n",
    "                                str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(lambda x: \" \".join(\n",
    "                                re.findall(\"[a-zA-Z]+\", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    overly complicated being both a us history buf...\n",
       "1    terrible disappointment parts dont fit my son ...\n",
       "2    didnt hold upvery disappointed i bought this o...\n",
       "3    gene hates jezebel i love jlj but this compila...\n",
       "4    nice toy but my sixyearold loves space and to ...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    my wife and i used to love arribas til recentl...\n",
       "1    you get what you pay for the food is inexpensi...\n",
       "2    unfortunately yesterdays visit was one of the ...\n",
       "3    i went into the scottsdale location yesterday ...\n",
       "4    it takes a lot for me to write a review and bl...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    once again mr costner has dragged out a movie ...\n",
       "1    first of all i would like to say that i am a f...\n",
       "2    im a huge fan of both emily watson breaking th...\n",
       "3    i was pulled into this movie early on much to ...\n",
       "4    this tale of the upperclasses getting their co...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_IMDB_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Amazon_Sentiment_Train = data_Preprocessing_Amazon_Train\n",
    "Clean_Yelp_Sentiment_Train = data_Preprocessing_Yelp_Train\n",
    "Clean_IMDB_Sentiment_Train = data_Preprocessing_IMDB_Train\n",
    "Label_Amazon_Train = data_train_amazon['Label']\n",
    "Label_Yelp_Train = data_train_yelp['Label']\n",
    "Label_IMDB_Train = data_train_imdb['Label']\n",
    "\n",
    "Clean_Amazon_Sentiment_Test = data_Preprocessing_Amazon_Test\n",
    "Clean_Yelp_Sentiment_Test = data_Preprocessing_Yelp_Test\n",
    "Clean_IMDB_Sentiment_Test = data_Preprocessing_IMDB_Test\n",
    "Label_Amazon_Test = data_test_amazon['Label']\n",
    "Label_Yelp_Test = data_test_yelp['Label']\n",
    "Label_IMDB_Test = data_test_imdb['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram (bigram dan trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_trigram(Clean_Sentiment) :\n",
    "    iterator = 0\n",
    "    Vocabulary = []\n",
    "    ngram_result = []\n",
    "    while iterator < len(Clean_Sentiment) :\n",
    "        ngram = 2\n",
    "        sentence = Clean_Sentiment[iterator]\n",
    "        vocab = list(ngrams(sentence, ngram))\n",
    "        temp = vocab\n",
    "\n",
    "        iterator2 = 0\n",
    "        while iterator2 < len (vocab):\n",
    "            if(vocab[iterator2] not in Vocabulary) :\n",
    "                Vocabulary.append(vocab[iterator2])\n",
    "            iterator2 = iterator2 + 1\n",
    "\n",
    "        ngram = 3\n",
    "        vocab = list(ngrams(sentence, ngram))\n",
    "        temp = temp + vocab\n",
    "        ngram_result.append(temp)\n",
    "        iterator2 = 0\n",
    "        while iterator2 < len (vocab):\n",
    "            if(vocab[iterator2] not in Vocabulary) :\n",
    "                Vocabulary.append(vocab[iterator2])\n",
    "            iterator2 = iterator2 + 1\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    return ngram_result,Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_amazon_train, Vocabulary_Amazon_Train = bigram_trigram(Clean_Amazon_Sentiment_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_yelp_train, Vocabulary_Yelp_Train = bigram_trigram(Clean_Yelp_Sentiment_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_amazon_test, Vocabulary_Amazon_Test = bigram_trigram(Clean_Amazon_Sentiment_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_yelp_test, Vocabulary_Yelp_Test = bigram_trigram(Clean_Yelp_Sentiment_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_imdb_train, Vocabulary_IMDB_Train = bigram_trigram(Clean_IMDB_Sentiment_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_imdb_test, Vocabulary_IMDB_Test = bigram_trigram(Clean_IMDB_Sentiment_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me inisialisasi jumlah frekuensi Gram di Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionaryInitialize(Clean_Sentiment, Vocabulary, ngram, jenisdata) :\n",
    "    wordDictionaryCount = []\n",
    "    iterator = 0\n",
    "    while iterator < len(Clean_Sentiment):\n",
    "        wordDictionaryCount.append(dict.fromkeys(Vocabulary , 0))\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    iterator = 0\n",
    "    \n",
    "    while iterator < len(Clean_Sentiment) :\n",
    "        for gram in ngram[iterator] :\n",
    "            if jenisdata == 'test' and gram not in Vocabulary :\n",
    "                continue\n",
    "            wordDictionaryCount[iterator][gram] += 1\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    return wordDictionaryCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Amazon_Train = dictionaryInitialize(Clean_Amazon_Sentiment_Train, Vocabulary_Amazon_Train, ngram_amazon_train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Yelp_Train = dictionaryInitialize(Clean_Yelp_Sentiment_Train, Vocabulary_Yelp_Train, ngram_yelp_train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_IMDB_Train = dictionaryInitialize(Clean_IMDB_Sentiment_Train, Vocabulary_IMDB_Train, ngram_imdb_train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Amazon_Test = dictionaryInitialize(Clean_Amazon_Sentiment_Test, Vocabulary_Amazon_Train, ngram_amazon_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Yelp_Test = dictionaryInitialize(Clean_Yelp_Sentiment_Test, Vocabulary_Yelp_Train, ngram_yelp_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_IMDB_Test = dictionaryInitialize(Clean_IMDB_Sentiment_Test, Vocabulary_IMDB_Train, ngram_imdb_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bu</th>\n",
       "      <th>uy</th>\n",
       "      <th>ye</th>\n",
       "      <th>er</th>\n",
       "      <th>r</th>\n",
       "      <th>b</th>\n",
       "      <th>be</th>\n",
       "      <th>ew</th>\n",
       "      <th>wa</th>\n",
       "      <th>ar</th>\n",
       "      <th>...</th>\n",
       "      <th>umy</th>\n",
       "      <th>myu</th>\n",
       "      <th>mmc</th>\n",
       "      <th>sez</th>\n",
       "      <th>fja</th>\n",
       "      <th>coz</th>\n",
       "      <th>ozy</th>\n",
       "      <th>kav</th>\n",
       "      <th>gju</th>\n",
       "      <th>ukw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 9321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bu  uy  ye  er  r    b  be  ew  wa  ar  ...  umy  myu  mmc  sez  fja  coz  \\\n",
       "0   1   1   1  11   9  13   5   3   4   7  ...    0    0    0    0    0    0   \n",
       "1   0   0   0   2   5   1   0   0   1   2  ...    0    0    0    0    0    0   \n",
       "\n",
       "   ozy  kav  gju  ukw  \n",
       "0    0    0    0    0  \n",
       "1    0    0    0    0  \n",
       "\n",
       "[2 rows x 9321 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Amazon_Train[0],WordDictionaryCount_Amazon_Train[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>d</th>\n",
       "      <th>do</th>\n",
       "      <th>on</th>\n",
       "      <th>nt</th>\n",
       "      <th>t</th>\n",
       "      <th>k</th>\n",
       "      <th>kn</th>\n",
       "      <th>no</th>\n",
       "      <th>ow</th>\n",
       "      <th>...</th>\n",
       "      <th>nhg</th>\n",
       "      <th>hgn</th>\n",
       "      <th>nwc</th>\n",
       "      <th>sez</th>\n",
       "      <th>ffb</th>\n",
       "      <th>fbe</th>\n",
       "      <th>yec</th>\n",
       "      <th>nww</th>\n",
       "      <th>zeg</th>\n",
       "      <th>ypu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 9399 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   i    d  do  on  nt  t    k  kn  no  ow  ...  nhg  hgn  nwc  sez  ffb  fbe  \\\n",
       "0   4  11   6  13   7  19   1   1   5   1  ...    0    0    0    0    0    0   \n",
       "1  10   7   6  10  13  21   1   1   1   1  ...    0    0    0    0    0    0   \n",
       "\n",
       "   yec  nww  zeg  ypu  \n",
       "0    0    0    0    0  \n",
       "1    0    0    0    0  \n",
       "\n",
       "[2 rows x 9399 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Yelp_Train[0],WordDictionaryCount_Yelp_Train[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>st</th>\n",
       "      <th>to</th>\n",
       "      <th>or</th>\n",
       "      <th>ry</th>\n",
       "      <th>y</th>\n",
       "      <th>o</th>\n",
       "      <th>of</th>\n",
       "      <th>f</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>ldk</th>\n",
       "      <th>dkr</th>\n",
       "      <th>ncz</th>\n",
       "      <th>oxn</th>\n",
       "      <th>xnu</th>\n",
       "      <th>tlt</th>\n",
       "      <th>pdu</th>\n",
       "      <th>duj</th>\n",
       "      <th>kao</th>\n",
       "      <th>aof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>81</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10327 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   st  to  or  ry  y    o  of  f    a  a   ...  ldk  dkr  ncz  oxn  xnu  tlt  \\\n",
       "0   7   5   6   2  10   9   5   5  12   9  ...    0    0    0    0    0    0   \n",
       "1  25  19  47   5  33  65  25  29  81  28  ...    0    0    0    0    0    0   \n",
       "\n",
       "   pdu  duj  kao  aof  \n",
       "0    0    0    0    0  \n",
       "1    0    0    0    0  \n",
       "\n",
       "[2 rows x 10327 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([WordDictionaryCount_IMDB_Train[0],WordDictionaryCount_IMDB_Train[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai TF pada setiap data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFrequency(wordDict, ngram):\n",
    "    freqword = {}\n",
    "    ngramLength = len(ngram)\n",
    "    for word, count in wordDict.items():\n",
    "        freqword[word] = count\n",
    "    return freqword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencyTermPerData(wordDictionary, ngram) :\n",
    "    CountFrequency = []\n",
    "    iterator = 0\n",
    "    while iterator < len(ngram):\n",
    "        CountFrequency.append(computeFrequency(wordDictionary[iterator], ngram[iterator]))\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    return CountFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountFrequency_Sentimen_Amazon_Train = getFrequencyTermPerData(WordDictionaryCount_Amazon_Train, ngram_amazon_train)\n",
    "CountFrequency_Sentimen_Yelp_Train = getFrequencyTermPerData(WordDictionaryCount_Yelp_Train, ngram_yelp_train)\n",
    "CountFrequency_Sentimen_IMDB_Train = getFrequencyTermPerData(WordDictionaryCount_IMDB_Train, ngram_imdb_train)\n",
    "CountFrequency_Sentimen_Amazon_Test = getFrequencyTermPerData(WordDictionaryCount_Amazon_Test, ngram_amazon_test)\n",
    "CountFrequency_Sentimen_Yelp_Test = getFrequencyTermPerData(WordDictionaryCount_Yelp_Test, ngram_yelp_test)\n",
    "CountFrequency_Sentimen_IMDB_Test = getFrequencyTermPerData(WordDictionaryCount_IMDB_Test, ngram_imdb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai IDF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(WordDict):\n",
    "    idfDict = {}\n",
    "    number_of_document_with_term_t_in_it = {}\n",
    "    N = len(WordDict)\n",
    "    \n",
    "    idfDict = dict.fromkeys(WordDict[0].keys(), 0)\n",
    "    for doc in WordDict:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        number_of_document_with_term_t_in_it[word] = float(val)\n",
    "        idfDict[word] = np.log((1+N) / (1+val)) + 1\n",
    "        \n",
    "    return idfDict, number_of_document_with_term_t_in_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_Amazon, number_of_document_with_term_t_Amazon_Train = computeIDF(WordDictionaryCount_Amazon_Train)\n",
    "IDF_Yelp, number_of_document_with_term_t_in_Yelp_Train = computeIDF(WordDictionaryCount_Yelp_Train)\n",
    "IDF_IMDB, number_of_document_with_term_t_in_IMDB_Train = computeIDF(WordDictionaryCount_IMDB_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDFnonScale(CountFrequency, idfs):\n",
    "    tfidf = None\n",
    "    tfidf = {}\n",
    "    for word, val in CountFrequency.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfidf_nonscaled) :\n",
    "    tfidf_nonscaled = np.array(tfidf_nonscaled)\n",
    "    tfidf_list = tfidf_nonscaled/sum(tfidf_nonscaled**2)**0.5\n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(wordDictionary, CountFrequency, IDF):\n",
    "    TFIDF_non_scaled = None\n",
    "    TFIDF_non_scaled = []\n",
    "    iterator = 0\n",
    "    while iterator < len(wordDictionary):\n",
    "        TFIDF_non_scaled.append(computeTFIDFnonScale(CountFrequency[iterator], IDF))\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    TFIDF = []\n",
    "    iterator = 0\n",
    "    while iterator < len(wordDictionary):\n",
    "        tf_idf_list = list(TFIDF_non_scaled[iterator].values())\n",
    "        TFIDF.append(computeTFIDF(tf_idf_list))\n",
    "        iterator = iterator + 1\n",
    "        \n",
    "    return TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Amazon_Train = getTFIDF(WordDictionaryCount_Amazon_Train, CountFrequency_Sentimen_Amazon_Train, IDF_Amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Amazon_Test = getTFIDF(WordDictionaryCount_Amazon_Test, CountFrequency_Sentimen_Amazon_Test, IDF_Amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Yelp_Train = getTFIDF(WordDictionaryCount_Yelp_Train, CountFrequency_Sentimen_Yelp_Train, IDF_Yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_IMDB_Train = getTFIDF(WordDictionaryCount_IMDB_Train, CountFrequency_Sentimen_IMDB_Train, IDF_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_IMDB_Test = getTFIDF(WordDictionaryCount_IMDB_Test, CountFrequency_Sentimen_IMDB_Test, IDF_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Yelp_Test = getTFIDF(WordDictionaryCount_Yelp_Test, CountFrequency_Sentimen_Yelp_Trai, IDF_Yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toVectorizeList(TFIDF):\n",
    "    iterator  = 0\n",
    "    Vectorized = []\n",
    "    while iterator < len(TFIDF):\n",
    "        Vectorized.append(list(TFIDF[iterator]))\n",
    "        iterator = iterator + 1\n",
    "    return Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFIDF_Yelp_Test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-db4324ab7884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mVectorizer_Amazon_Test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoVectorizeList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFIDF_Amazon_Test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mVectorizer_Yelp_Train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoVectorizeList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFIDF_Yelp_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mVectorizer_Yelp_Test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoVectorizeList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFIDF_Yelp_Test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mVectorizer_IMDB_Train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoVectorizeList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFIDF_IMDB_Train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mVectorizer_IMDB_Test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoVectorizeList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFIDF_IMDB_Test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TFIDF_Yelp_Test' is not defined"
     ]
    }
   ],
   "source": [
    "Vectorizer_Amazon_Train = toVectorizeList(TFIDF_Amazon_Train)\n",
    "Vectorizer_Amazon_Test = toVectorizeList(TFIDF_Amazon_Test)\n",
    "Vectorizer_Yelp_Train = toVectorizeList(TFIDF_Yelp_Train)\n",
    "Vectorizer_Yelp_Test = toVectorizeList(TFIDF_Yelp_Test)\n",
    "Vectorizer_IMDB_Train = toVectorizeList(TFIDF_IMDB_Train)\n",
    "Vectorizer_IMDB_Test = toVectorizeList(TFIDF_IMDB_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Amazon_Train = pd.DataFrame(Vectorizer_Amazon_Train, columns = Vocabulary_Amazon_Train)\n",
    "DataFrame_Amazon_Train = DataFrame_Amazon_Train.reindex(sorted(DataFrame_Amazon_Train.columns), axis=1)\n",
    "print (DataFrame_Amazon_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Amazon_Test = pd.DataFrame(Vectorizer_Amazon_Test, columns = Vocabulary_Amazon_Train)\n",
    "DataFrame_Amazon_Test = DataFrame_Amazon_Test.reindex(sorted(DataFrame_Amazon_Test.columns), axis=1)\n",
    "print (DataFrame_Amazon_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Yelp_Train = pd.DataFrame(Vectorizer_Yelp_Train, columns = Vocabulary_Yelp_Train)\n",
    "DataFrame_Yelp_Train = DataFrame_Yelp_Train.reindex(sorted(DataFrame_Yelp_Train.columns), axis=1)\n",
    "print (DataFrame_Yelp_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Yelp_Test = pd.DataFrame(Vectorizer_Yelp_Test, columns = Vocabulary_Yelp_Train)\n",
    "DataFrame_Yelp_Test = DataFrame_Yelp_Test.reindex(sorted(DataFrame_Yelp_Test.columns), axis=1)\n",
    "print (DataFrame_Yelp_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_IMDB_Train = pd.DataFrame(Vectorizer_IMDB_Train, columns = Vocabulary_IMDB_Train)\n",
    "DataFrame_IMDB_Train = DataFrame_IMDB_Train.reindex(sorted(DataFrame_IMDB_Train.columns), axis=1)\n",
    "print (DataFrame_IMDB_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_IMDB_Test = pd.DataFrame(Vectorizer_IMDB_Test, columns = Vocabulary_IMDB_Train)\n",
    "DataFrame_IMDB_Test = DataFrame_IMDB_Test.reindex(sorted(DataFrame_IMDB_Test.columns), axis=1)\n",
    "print (DataFrame_IMDB_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akurasi dan Waktu Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_Amazon = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Amazon.fit(Vectorizer_Amazon_Train, Label_Amazon_Train)\n",
    "print(f\"\\nWaktu Training Amazon: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Amazon = RF_Classifier_Amazon.score(Vectorizer_Amazon_Train, Label_Amazon_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Amazon = RF_Classifier_Amazon.score(Vectorizer_Amazon_Test, Label_Amazon_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Amazon : {}\".format(Skor_Train_Amazon))\n",
    "print(\"Skor Random Forest Test Amazon : {}\".format(Skor_Test_Amazon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_Yelp = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Yelp.fit(Vectorizer_Yelp_Train, Label_Yelp_Train)\n",
    "print(f\"\\nWaktu Training Yelp: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Yelp = RF_Classifier_Yelp.score(Vectorizer_Yelp_Train, Label_Yelp_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Yelp = RF_Classifier_Yelp.score(Vectorizer_Yelp_Test, Label_Yelp_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Yelp : {}\".format(Skor_Train_Yelp))\n",
    "print(\"Skor Random Forest Test Yelp : {}\".format(Skor_Test_Yelp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_IMDB = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_IMDB.fit(Vectorizer_IMDB_Train, Label_IMDB_Train)\n",
    "print(f\"\\nWaktu Training IMDB: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_IMDB = RF_Classifier_IMDB.score(Vectorizer_IMDB_Train, Label_IMDB_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_IMDB = RF_Classifier_IMDB.score(Vectorizer_IMDB_Test, Label_IMDB_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train IMDB : {}\".format(Skor_Train_IMDB))\n",
    "print(\"Skor Random Forest Test IMDB : {}\".format(Skor_Test_IMDB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mencari Term-Term yang interseksi antara Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Intersection = []\n",
    "iterator = 0\n",
    "while iterator < len(Vocabulary_Yelp_Train) :\n",
    "    if Vocabulary_Yelp_Train[iterator] in Vocabulary_Amazon_Train :\n",
    "        Vocabulary_Intersection.append(Vocabulary_Yelp_Train[iterator])\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecomputeIDF(WordDictAmazon, WordDictYelp, additional_term_value, Vocabulary_Intersection):\n",
    "    idfDict = {}\n",
    "    number_of_document_with_term_t_in_it = {}\n",
    "    N = len(WordDictYelp) + len(WordDictAmazon)\n",
    "    idfDict = dict.fromkeys(WordDictYelp[0].keys(), 0)\n",
    "    for doc in WordDictYelp:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        if word in Vocabulary_Intersection :\n",
    "            val = val + additional_term_value[word]\n",
    "        idfDict[word] = np.log((1+N) / (1+val)) + 1\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_IDF_Yelp = RecomputeIDF(WordDictionaryCount_Amazon_Train, WordDictionaryCount_Yelp_Train, number_of_document_with_term_t_Amazon_Train, Vocabulary_Intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_TFIDF_nonScale_Yelp_Train = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp_Train):\n",
    "    New_TFIDF_nonScale_Yelp_Train.append(computeTFIDFnonScale(CountFrequency_Sentimen_Yelp_Train[iterator], New_IDF_Yelp))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_TFIDF_Yelp_Train = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp_Train):\n",
    "    tf_idf_list = list(New_TFIDF_nonScale_Yelp_Train[iterator].values())\n",
    "    New_TFIDF_Yelp_Train.append(computeTFIDF(tf_idf_list))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Vectorizer_Yelp_Train = []\n",
    "iterator  = 0\n",
    "while iterator < len(New_TFIDF_Yelp_Train):\n",
    "    New_Vectorizer_Yelp_Train.append(list(New_TFIDF_Yelp_Train[iterator]))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Training = time()\n",
    "New_RF_Classifier_Yelp = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "New_RF_Classifier_Yelp.fit(New_Vectorizer_Yelp_Train, Label_Yelp_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "New_Skor_Train_Yelp = New_RF_Classifier_Yelp.score(Vectorizer_Yelp_Train, Label_Yelp_Train)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "print(\"Skor Random Forest Test Yelp : {}\".format(New_Skor_Train_Yelp))\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "New_Skor_Test_Yelp = New_RF_Classifier_Yelp.score(Vectorizer_Yelp_Test, Label_Yelp_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "print(\"Skor Random Forest Test Yelp : {}\".format(New_Skor_Test_Yelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
