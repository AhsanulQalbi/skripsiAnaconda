{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKRIPSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import csv \n",
    "from nltk import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, validation_curve,RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from matplotlib_venn import venn2\n",
    "from time import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(0)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "poster_Stemmer = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baca csv\n",
    "data_train_amazon = pd.read_csv('dataset/Amazon_Train.csv')\n",
    "data_train_yelp = pd.read_csv('dataset/Yelp_Train.csv')\n",
    "data_test_amazon = pd.read_csv('dataset/Amazon_Test.csv')\n",
    "data_test_yelp = pd.read_csv('dataset/Yelp_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_amazon.head())\n",
    "print(\"\\n\")\n",
    "print(data_train_yelp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test_amazon.head())\n",
    "print(\"\\n\")\n",
    "print(data_test_yelp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing ke lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_Preprocessing_Amazon_Train = data_train_amazon\n",
    "data_Lowercase_Amazon_Train = []\n",
    "\n",
    "data_Preprocessing_Yelp_Train = data_train_yelp\n",
    "data_Lowercase_Yelp_Train = []\n",
    "\n",
    "while iterator < len(data_train_amazon) :\n",
    "    data_Lowercase_Amazon_Train.append(data_train_amazon.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_train_yelp) :\n",
    "    data_Lowercase_Yelp_Train.append(data_train_yelp.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_Preprocessing_Amazon_Test = data_test_amazon\n",
    "data_Lowercase_Amazon_Test = []\n",
    "\n",
    "data_Preprocessing_Yelp_Test = data_test_yelp\n",
    "data_Lowercase_Yelp_Test = []\n",
    "\n",
    "while iterator < len(data_test_amazon) :\n",
    "    data_Lowercase_Amazon_Test.append(data_test_amazon.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_test_yelp) :\n",
    "    data_Lowercase_Yelp_Test.append(data_test_yelp.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['Lowercase'] = data_Lowercase_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['Lowercase'] = data_Lowercase_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['Lowercase'] = data_Lowercase_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['Lowercase'] = data_Lowercase_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemoveNumber_Amazon_Train = []\n",
    "\n",
    "data_RemoveNumber_Yelp_Train = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Train) :\n",
    "    data_RemoveNumber_Amazon_Train.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Amazon_Train.Lowercase[iterator]))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Train) :\n",
    "    data_RemoveNumber_Yelp_Train.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Yelp_Train.Lowercase[iterator]))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemoveNumber_Amazon_Test = []\n",
    "\n",
    "data_RemoveNumber_Yelp_Test = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Test) :\n",
    "    data_RemoveNumber_Amazon_Test.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Amazon_Test.Lowercase[iterator]))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Test) :\n",
    "    data_RemoveNumber_Yelp_Test.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Yelp_Test.Lowercase[iterator]))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['RemoveNumber'] = data_RemoveNumber_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['RemoveNumber'] = data_RemoveNumber_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['RemoveNumber'] = data_RemoveNumber_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['RemoveNumber'] = data_RemoveNumber_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan tanda baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemovePunctuation_Amazon_Train = []\n",
    "\n",
    "data_RemovePunctuation_Yelp_Train = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Train) :\n",
    "    data_RemovePunctuation_Amazon_Train.append(data_Preprocessing_Amazon_Train.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Train) :\n",
    "    data_RemovePunctuation_Yelp_Train.append(data_Preprocessing_Yelp_Train.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemovePunctuation_Amazon_Test = []\n",
    "\n",
    "data_RemovePunctuation_Yelp_Test = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Test) :\n",
    "    data_RemovePunctuation_Amazon_Test.append(data_Preprocessing_Amazon_Test.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Test) :\n",
    "    data_RemovePunctuation_Yelp_Test.append(data_Preprocessing_Yelp_Test.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['RemovePunctuation'] = data_RemovePunctuation_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['RemovePunctuation'] = data_RemovePunctuation_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['RemovePunctuation'] = data_RemovePunctuation_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['RemovePunctuation'] = data_RemovePunctuation_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan Non alfabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "\n",
    "data_Regex_alpabet_only_Amazon_Train = []\n",
    "\n",
    "data_Regex_alpabet_only_Yelp_Train = []\n",
    "\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Train) :\n",
    "    data_Regex_alpabet_only_Amazon_Train.append(\" \".join(re.findall(\"[a-zA-Z]+\", data_Preprocessing_Amazon_Train.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Train) :\n",
    "    data_Regex_alpabet_only_Yelp_Train.append(\" \".join(re.findall(r\"[a-zA-Z]+\", data_Preprocessing_Yelp_Train.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "\n",
    "data_Regex_alpabet_only_Amazon_Test = []\n",
    "\n",
    "data_Regex_alpabet_only_Yelp_Test = []\n",
    "\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Test) :\n",
    "    data_Regex_alpabet_only_Amazon_Test.append(\" \".join(re.findall(\"[a-zA-Z]+\", data_Preprocessing_Amazon_Test.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Test) :\n",
    "    data_Regex_alpabet_only_Yelp_Test.append(\" \".join(re.findall(r\"[a-zA-Z]+\", data_Preprocessing_Yelp_Test.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['Regex'] = data_Regex_alpabet_only_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['Regex'] = data_Regex_alpabet_only_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['Regex'] = data_Regex_alpabet_only_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['Regex'] = data_Regex_alpabet_only_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Amazon_Sentiment_Train = data_Preprocessing_Amazon_Train['Regex']\n",
    "Clean_Yelp_Sentiment_Train = data_Preprocessing_Yelp_Train['Regex']\n",
    "Label_Amazon_Train = data_Preprocessing_Amazon_Train['Label']\n",
    "Label_Yelp_Train = data_Preprocessing_Yelp_Train['Label']\n",
    "\n",
    "Clean_Amazon_Sentiment_Test = data_Preprocessing_Amazon_Test['Regex']\n",
    "Clean_Yelp_Sentiment_Test = data_Preprocessing_Yelp_Test['Regex']\n",
    "Label_Amazon_Test = data_Preprocessing_Amazon_Test['Label']\n",
    "Label_Yelp_Test = data_Preprocessing_Yelp_Test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = tfidfvectorizer()\n",
    "# tf.fit(train)\n",
    "# trainx = tf.transform(train)\n",
    "# testx = tf.transform(test)\n",
    "\n",
    "vectorizer_Amazon = TfidfVectorizer(ngram_range=(2,3), analyzer = 'char')\n",
    "vectorizer_Amazon.fit(Clean_Amazon_Sentiment_Train)\n",
    "Data_Train_Amazon = vectorizer_Amazon.transform(Clean_Amazon_Sentiment_Train).toarray()\n",
    "Vocabulary_Train_Amazon = vectorizer_Amazon.get_feature_names()\n",
    "Data_Test_Amazon = vectorizer_Amazon.transform(Clean_Amazon_Sentiment_Test).toarray()\n",
    "\n",
    "vectorizer_Yelp = TfidfVectorizer(ngram_range=(2,3),analyzer = 'char' )\n",
    "vectorizer_Yelp.fit(Clean_Yelp_Sentiment_Train)\n",
    "Data_Train_Yelp = vectorizer_Yelp.transform(Clean_Yelp_Sentiment_Train).toarray()\n",
    "Vocabulary_Train_Yelp = vectorizer_Yelp.get_feature_names()\n",
    "Data_Test_Yelp = vectorizer_Yelp.transform(Clean_Yelp_Sentiment_Test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Amazon_Train = pd.DataFrame(Data_Train_Amazon, columns = Vocabulary_Train_Amazon)\n",
    "print (DataFrame_Amazon_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Amazon_Test = pd.DataFrame(Data_Test_Amazon, columns = Vocabulary_Train_Amazon)\n",
    "print (DataFrame_Amazon_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Yelp_Train = pd.DataFrame(Data_Train_Yelp, columns = Vocabulary_Train_Yelp)\n",
    "print (DataFrame_Yelp_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Yelp_Test = pd.DataFrame(Data_Test_Yelp, columns = Vocabulary_Train_Yelp)\n",
    "print (DataFrame_Yelp_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_Amazon = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Amazon.fit(Data_Train_Amazon, Label_Amazon_Train)\n",
    "print(f\"\\nWaktu Training Amazon: {round(time()-Waktu_Training, 3)}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Amazon = RF_Classifier_Amazon.score(Data_Train_Amazon, Label_Amazon_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Amazon = RF_Classifier_Amazon.score(Data_Test_Amazon, Label_Amazon_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Amazon : {}\".format(Skor_Train_Amazon))\n",
    "print(\"Skor Random Forest Test Amazon : {}\\n\\n\".format(Skor_Test_Amazon))\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Amazon.predict(Data_Train_Amazon)\n",
    "Confusion_matrix = confusion_matrix(Label_Amazon_Train, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Train: \", accuracy_score(Label_Amazon_Train, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Amazon_Train, RFC_predict))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Amazon.predict(Data_Test_Amazon)\n",
    "Confusion_matrix = confusion_matrix(Label_Amazon_Test, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy TEST: \", accuracy_score(Label_Amazon_Test, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Amazon_Test, RFC_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_Yelp = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Yelp.fit(Data_Train_Yelp, Label_Yelp_Train)\n",
    "print(f\"\\nWaktu Training Yelp: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Yelp = RF_Classifier_Yelp.score(Data_Train_Yelp, Label_Yelp_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Yelp = RF_Classifier_Yelp.score(Data_Test_Yelp, Label_Yelp_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Yelp : {}\".format(Skor_Train_Yelp))\n",
    "print(\"Skor Random Forest Test Yelp : {}\".format(Skor_Test_Yelp))\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Yelp.predict(Data_Train_Yelp)\n",
    "Confusion_matrix = confusion_matrix(Label_Yelp_Train, RFC_predict)\n",
    "print(\"\\n\\n\")\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Train: \", accuracy_score(Label_Yelp_Train, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Yelp_Train, RFC_predict))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Yelp.predict(Data_Test_Yelp)\n",
    "Confusion_matrix = confusion_matrix(Label_Yelp_Test, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Test: \", accuracy_score(Label_Yelp_Test, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Yelp_Test, RFC_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Importance_Amazon = []\n",
    "iterator = 0\n",
    "length = len (Vocabulary_Train_Amazon)\n",
    "\n",
    "while iterator < length : \n",
    "    if RF_Classifier_Amazon.feature_importances_[iterator] > 0 :\n",
    "        Vocabulary_Importance_Amazon.append(Vocabulary_Train_Amazon[iterator])\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat Vectorizer baru dari data fitur importance only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_importance_amazon = TfidfVectorizer(vocabulary = Vocabulary_Importance_Amazon, ngram_range=(2,3),analyzer = 'char')\n",
    "\n",
    "vectorizer_importance_amazon.fit(Clean_Amazon_Sentiment_Train)\n",
    "Vocabulary_Importance_Amazon_Train = vectorizer_importance_amazon.get_feature_names()\n",
    "\n",
    "Data_Train_Importance_Amazon = vectorizer_importance_amazon.transform(Clean_Amazon_Sentiment_Train).toarray()\n",
    "Data_Test_Importance_Amazon = vectorizer_importance_amazon.transform(Clean_Amazon_Sentiment_Test).toarray()\n",
    "\n",
    "# tf = tfidfvectorizer()\n",
    "# tf.fit(train)\n",
    "# trainx = tf.transform(train)\n",
    "# testx = tf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Importance_Amazon = pd.DataFrame(Data_Train_Importance_Amazon, columns = Vocabulary_Importance_Amazon_Train)\n",
    "print (DataFrame_Importance_Amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Importance_Amazon_Test = pd.DataFrame(Data_Test_Importance_Amazon, columns = Vocabulary_Importance_Amazon_Train)\n",
    "print (DataFrame_Importance_Amazon_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_Amazon_Importance = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Amazon_Importance.fit(Data_Train_Importance_Amazon, Label_Amazon_Train)\n",
    "print(f\"\\nWaktu Training Amazon Feature Importance Only: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Amazon_FI = RF_Classifier_Amazon_Importance.score(Data_Train_Importance_Amazon, Label_Amazon_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Amazon_FI = RF_Classifier_Amazon_Importance.score(Data_Test_Importance_Amazon, Label_Amazon_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Amazon Feature Importance : {}\".format(Skor_Train_Amazon_FI))\n",
    "print(\"Skor Random Forest Test Amazon Feature Importance : {}\".format(Skor_Test_Amazon_FI))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Amazon_Importance.predict(Data_Train_Importance_Amazon)\n",
    "Confusion_matrix = confusion_matrix(Label_Amazon_Train, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Train: \", accuracy_score(Label_Amazon_Train, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Amazon_Train, RFC_predict))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Amazon_Importance.predict(Data_Test_Importance_Amazon)\n",
    "Confusion_matrix = confusion_matrix(Label_Amazon_Test, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Test: \", accuracy_score(Label_Amazon_Test, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Amazon_Test, RFC_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengetahui akurasi dengan hanya mengambil data train dari interseksi antara data yelp dan amazon(Feature imporantace > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_intersection = list(set(\n",
    "                            Vocabulary_Importance_Amazon) & set (\n",
    "                            Vocabulary_Train_Yelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn2(subsets = (len(Vocabulary_Importance_Amazon_Train), len(Vocabulary_Train_Yelp), len(Vocabulary_intersection)), set_labels = ('Fitur Model acuan yang fiture importance > 0', 'Vocabulary Model baru'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_intersection_yelp = TfidfVectorizer(vocabulary = Vocabulary_intersection, ngram_range=(2,3),analyzer = 'char')\n",
    "vectorizer_intersection_yelp.fit(Clean_Yelp_Sentiment_Train)\n",
    "\n",
    "Vocabulary_Intersection_Train = vectorizer_intersection_yelp.get_feature_names()\n",
    "Data_Train_Intersection = vectorizer_intersection_yelp.transform(Clean_Yelp_Sentiment_Train).toarray()\n",
    "Data_Test_Intersection = vectorizer_intersection_yelp.transform(Clean_Yelp_Sentiment_Test).toarray()\n",
    "\n",
    "# tf = tfidfvectorizer()\n",
    "# tf.fit(train)\n",
    "# trainx = tf.transform(train)\n",
    "# testx = tf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Intersection_Train = pd.DataFrame(Data_Train_Intersection, columns = Vocabulary_Intersection_Train)\n",
    "print(DataFrame_Intersection_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Akurasi interseksi yelp dan amazon (FI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waktu_training_interseksi = time()\n",
    "RF_Classifier_Yelp_intersec = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Yelp_intersec.fit(Data_Train_Intersection, Label_Yelp_Train)\n",
    "print(f\"\\nWaktu Training data interseksi Only: {round(time()-waktu_training_interseksi, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Interseksi_Yelp = RF_Classifier_Yelp_intersec.score(Data_Train_Intersection, Label_Yelp_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Interseksi_Yelp = RF_Classifier_Yelp_intersec.score(Data_Test_Intersection, Label_Yelp_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Interseksi : {}\".format(Skor_Train_Interseksi_Yelp))\n",
    "print(\"Skor Random Forest Test Interseksi : {}\".format(Skor_Test_Interseksi_Yelp))\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Yelp_intersec.predict(Data_Train_Intersection)\n",
    "Confusion_matrix = confusion_matrix(Label_Yelp_Train, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Train: \", accuracy_score(Label_Yelp_Train, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Yelp_Train, RFC_predict))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Yelp_intersec.predict(Data_Test_Intersection)\n",
    "Confusion_matrix = confusion_matrix(Label_Yelp_Test, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Test: \", accuracy_score(Label_Yelp_Test, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Yelp_Test, RFC_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengetahui akurasi dengan hanya mengambil Data Train Yelp yang diseleksi dengan data amazon feature importance > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meseleksi jika terdapat term yang sama pada dictionary yelp dan amazon maka akan di cek apakah term tersebut, \n",
    "# memiliki feature importance > 0, jika tidak maka tidak akan dimasukan untuk data train selanjutnya  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Seleksi_Yelp = []\n",
    "iterator = 0\n",
    "length = len(Vocabulary_Train_Yelp)\n",
    "\n",
    "while iterator < length : \n",
    "#     jika interseksi dengan amazon dan bukan fitur importance maka vocab tidak dimasukan\n",
    "    if  (Vocabulary_Train_Yelp[iterator] in Vocabulary_Train_Amazon) and (Vocabulary_Train_Yelp[iterator] not in Vocabulary_Importance_Amazon_Train) : \n",
    "        iterator = iterator + 1\n",
    "        continue\n",
    "        \n",
    "    Vocabulary_Seleksi_Yelp.append(Vocabulary_Train_Yelp[iterator])\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_seleksi_yelp = TfidfVectorizer(vocabulary = Vocabulary_Seleksi_Yelp, ngram_range=(2,3),analyzer = 'char')\n",
    "vectorizer_seleksi_yelp.fit(Clean_Yelp_Sentiment_Train)\n",
    "\n",
    "Vocabulary_Seleksi_Yelp_Train = vectorizer_seleksi_yelp.get_feature_names()\n",
    "Data_Train_Seleksi_Yelp = vectorizer_seleksi_yelp.transform(Clean_Yelp_Sentiment_Train).toarray()\n",
    "Data_Test_Seleksi_Yelp = vectorizer_seleksi_yelp.transform(Clean_Yelp_Sentiment_Test).toarray()\n",
    "\n",
    "# tf = tfidfvectorizer()\n",
    "# tf.fit(train)\n",
    "# trainx = tf.transform(train)\n",
    "# testx = tf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Seleksi = pd.DataFrame(Data_Train_Seleksi_Yelp, columns = Vocabulary_Seleksi_Yelp_Train)\n",
    "print (DataFrame_Seleksi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waktu_training_seleksi_yelp = time()\n",
    "RF_Classifier_Yelp_Seleksi = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Yelp_Seleksi.fit(Data_Train_Seleksi_Yelp, Label_Yelp_Train)\n",
    "print(f\"\\nWaktu Training data seleksi: {round(time()-waktu_training_seleksi_yelp, 3)}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Seleksi_Yelp = RF_Classifier_Yelp_Seleksi.score(Data_Train_Seleksi_Yelp, Label_Yelp_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Seleksi_Yelp = RF_Classifier_Yelp_Seleksi.score(Data_Test_Seleksi_Yelp, Label_Yelp_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Seleksi : {}\".format(Skor_Train_Seleksi_Yelp))\n",
    "print(\"Skor Random Forest Test Seleksi : {}\".format(Skor_Test_Seleksi_Yelp))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Yelp_Seleksi.predict(Data_Train_Seleksi_Yelp)\n",
    "Confusion_matrix = confusion_matrix(Label_Yelp_Train, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy Test: \", accuracy_score(Label_Yelp_Train, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Yelp_Train, RFC_predict))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFC_predict = RF_Classifier_Yelp_Seleksi.predict(Data_Test_Seleksi_Yelp)\n",
    "Confusion_matrix = confusion_matrix(Label_Yelp_Test, RFC_predict)\n",
    "print(Confusion_matrix)\n",
    "print(\"\\nAccuracy TEST: \", accuracy_score(Label_Yelp_Test, RFC_predict))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(Label_Yelp_Test, RFC_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(RF_Classifier_Yelp_Seleksi, 'RF_seleksi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(vectorizer_seleksi_yelp, 'vectroizer_seleksi.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_tes = pd.read_csv('tes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_tes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfprint = pd.DataFrame()\n",
    "dfprint['COMMENT'] = data_train_tes.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_baru = dfprint['COMMENT'].astype(str)    \n",
    "text_baru = text_baru.apply(lambda x: x.lower()) #Lower Case\n",
    "text_baru = text_baru.apply(lambda x: re.sub(r\"\\d\", \"\", x)) #Remove Number    \n",
    "text_baru = text_baru.apply(lambda x: x.translate(str.maketrans('','',string.punctuation))) #punctuation  \n",
    "text_baru = text_baru.apply(lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_baru[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (text_baru[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_tes.sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = \"``\".join([str(i) for i in data_train_tes.columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_tes[cols][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
