{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKRIPSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, validation_curve,RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from matplotlib_venn import venn2\n",
    "from time import time\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(0)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "poster_Stemmer = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baca csv\n",
    "data_train_amazon = pd.read_csv('dataset/Amazon_Train.csv')\n",
    "data_test_amazon = pd.read_csv('dataset/Amazon_Test.csv')\n",
    "data_train_imdb = pd.read_csv('dataset/IMDB_Train_1000.csv')\n",
    "data_test_imdb = pd.read_csv('dataset/IMDB_Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train = data_train_amazon.Sentimen.astype(str)\n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(lambda x: x.lower())\n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(\n",
    "                                    lambda x: x.translate(str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_Amazon_Train = data_Preprocessing_Amazon_Train.apply(\n",
    "                                    lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n",
    "data_Preprocessing_IMDB_Train = data_train_imdb.Sentimen.astype(str)\n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(lambda x: x.lower())\n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(\n",
    "                                    lambda x: x.translate(str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_IMDB_Train = data_Preprocessing_IMDB_Train.apply(\n",
    "                                    lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    buyer beware this is a selfpublished book and ...\n",
       "1    the worst a complete waste of time typographic...\n",
       "2    oh please i guess you have to be a romance nov...\n",
       "3    awful beyond belief i feel i have to write to ...\n",
       "4    another abysmal digital copy rather than scrat...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    story of a man who has unnatural feelings for ...\n",
       "1    airport starts as a brand new luxury plane is ...\n",
       "2    this film lacked something i couldnt put my fi...\n",
       "3    sorry everyone i know this is supposed to be a...\n",
       "4    when i was little my parents took me along to ...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_IMDB_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test = data_test_amazon.Sentimen.astype(str)\n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: x.lower())\n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: x.translate(\n",
    "                                str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_Amazon_Test = data_Preprocessing_Amazon_Test.apply(lambda x: \" \".join(\n",
    "                                re.findall(\"[a-zA-Z]+\", x)))\n",
    "\n",
    "data_Preprocessing_IMDB_Test = data_test_imdb.Sentimen.astype(str)\n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(lambda x: x.lower())\n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(lambda x: re.sub(r\"\\d\", \"\", x))   \n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(\n",
    "                                    lambda x: x.translate(str.maketrans('','',string.punctuation)))  \n",
    "data_Preprocessing_IMDB_Test = data_Preprocessing_IMDB_Test.apply(\n",
    "                                    lambda x: \" \".join(re.findall(\"[a-zA-Z]+\", x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    overly complicated being both a us history buf...\n",
       "1    terrible disappointment parts dont fit my son ...\n",
       "2    didnt hold upvery disappointed i bought this o...\n",
       "3    gene hates jezebel i love jlj but this compila...\n",
       "4    nice toy but my sixyearold loves space and to ...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    once again mr costner has dragged out a movie ...\n",
       "1    first of all i would like to say that i am a f...\n",
       "2    im a huge fan of both emily watson breaking th...\n",
       "3    i was pulled into this movie early on much to ...\n",
       "4    this tale of the upperclasses getting their co...\n",
       "Name: Sentimen, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Preprocessing_IMDB_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Amazon_Sentiment_Train = data_Preprocessing_Amazon_Train\n",
    "Clean_IMDB_Sentiment_Train = data_Preprocessing_IMDB_Train\n",
    "Label_Amazon_Train = data_train_amazon['Label']\n",
    "Label_IMDB_Train = data_train_imdb['Label']\n",
    "\n",
    "Clean_Amazon_Sentiment_Test = data_Preprocessing_Amazon_Test\n",
    "Clean_IMDB_Sentiment_Test = data_Preprocessing_IMDB_Test\n",
    "Label_Amazon_Test = data_test_amazon['Label']\n",
    "Label_IMDB_Test = data_test_imdb['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram (bigram dan trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_trigram(Clean_Sentiment) :\n",
    "    iterator = 0\n",
    "    Vocabulary = []\n",
    "    ngram_result = []\n",
    "    while iterator < len(Clean_Sentiment) :\n",
    "        ngram = 2\n",
    "        sentence = Clean_Sentiment[iterator]\n",
    "        vocab = list(ngrams(sentence, ngram))\n",
    "        temp = vocab\n",
    "\n",
    "        iterator2 = 0\n",
    "        while iterator2 < len (vocab):\n",
    "            if(vocab[iterator2] not in Vocabulary) :\n",
    "                Vocabulary.append(vocab[iterator2])\n",
    "            iterator2 = iterator2 + 1\n",
    "\n",
    "        ngram = 3\n",
    "        vocab = list(ngrams(sentence, ngram))\n",
    "        temp = temp + vocab\n",
    "        ngram_result.append(temp)\n",
    "        iterator2 = 0\n",
    "        while iterator2 < len (vocab):\n",
    "            if(vocab[iterator2] not in Vocabulary) :\n",
    "                Vocabulary.append(vocab[iterator2])\n",
    "            iterator2 = iterator2 + 1\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    return ngram_result,Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_amazon_train, Vocabulary_Amazon_Train = bigram_trigram(Clean_Amazon_Sentiment_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_amazon_test, Vocabulary_Amazon_Test = bigram_trigram(Clean_Amazon_Sentiment_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_imdb_train, Vocabulary_IMDB_Train = bigram_trigram(Clean_IMDB_Sentiment_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_imdb_test, Vocabulary_IMDB_Test = bigram_trigram(Clean_IMDB_Sentiment_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me inisialisasi jumlah frekuensi Gram di Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionaryInitialize(Clean_Sentiment, Vocabulary, ngram, jenisdata) :\n",
    "    wordDictionaryCount = []\n",
    "    iterator = 0\n",
    "    while iterator < len(Clean_Sentiment):\n",
    "        wordDictionaryCount.append(dict.fromkeys(Vocabulary , 0))\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    iterator = 0\n",
    "    \n",
    "    while iterator < len(Clean_Sentiment) :\n",
    "        for gram in ngram[iterator] :\n",
    "            if jenisdata == 'test' and gram not in Vocabulary :\n",
    "                continue\n",
    "            wordDictionaryCount[iterator][gram] += 1\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    return wordDictionaryCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Amazon_Train = dictionaryInitialize(Clean_Amazon_Sentiment_Train, Vocabulary_Amazon_Train, ngram_amazon_train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Amazon_Test = dictionaryInitialize(Clean_Amazon_Sentiment_Test, Vocabulary_Amazon_Train, ngram_amazon_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_IMDB_Test = dictionaryInitialize(Clean_IMDB_Sentiment_Test, Vocabulary_IMDB_Train, ngram_imdb_test,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_IMDB_Train = dictionaryInitialize(Clean_IMDB_Sentiment_Train, Vocabulary_IMDB_Train, ngram_imdb_train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(b, u)</th>\n",
       "      <th>(u, y)</th>\n",
       "      <th>(y, e)</th>\n",
       "      <th>(e, r)</th>\n",
       "      <th>(r,  )</th>\n",
       "      <th>( , b)</th>\n",
       "      <th>(b, e)</th>\n",
       "      <th>(e, w)</th>\n",
       "      <th>(w, a)</th>\n",
       "      <th>(a, r)</th>\n",
       "      <th>...</th>\n",
       "      <th>(k, o, y)</th>\n",
       "      <th>(t, y, n)</th>\n",
       "      <th>(w, s, y)</th>\n",
       "      <th>(i, u, n)</th>\n",
       "      <th>(b, y, p)</th>\n",
       "      <th>(b, u, o)</th>\n",
       "      <th>( , u, e)</th>\n",
       "      <th>(u, e, m)</th>\n",
       "      <th>(a, b, w)</th>\n",
       "      <th>(m, u,  )</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 8181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (b, u)  (u, y)  (y, e)  (e, r)  (r,  )  ( , b)  (b, e)  (e, w)  (w, a)  \\\n",
       "0       1       1       1      11       9      13       5       3       4   \n",
       "1       0       0       0       2       5       1       0       0       1   \n",
       "\n",
       "   (a, r)  ...  (k, o, y)  (t, y, n)  (w, s, y)  (i, u, n)  (b, y, p)  \\\n",
       "0       7  ...          0          0          0          0          0   \n",
       "1       2  ...          0          0          0          0          0   \n",
       "\n",
       "   (b, u, o)  ( , u, e)  (u, e, m)  (a, b, w)  (m, u,  )  \n",
       "0          0          0          0          0          0  \n",
       "1          0          0          0          0          0  \n",
       "\n",
       "[2 rows x 8181 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Amazon_Train[0],WordDictionaryCount_Amazon_Train[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(s, t)</th>\n",
       "      <th>(t, o)</th>\n",
       "      <th>(o, r)</th>\n",
       "      <th>(r, y)</th>\n",
       "      <th>(y,  )</th>\n",
       "      <th>( , o)</th>\n",
       "      <th>(o, f)</th>\n",
       "      <th>(f,  )</th>\n",
       "      <th>( , a)</th>\n",
       "      <th>(a,  )</th>\n",
       "      <th>...</th>\n",
       "      <th>(z, v, o)</th>\n",
       "      <th>(d, o, v)</th>\n",
       "      <th>(k, r, e)</th>\n",
       "      <th>(f, b, a)</th>\n",
       "      <th>(i, d, y)</th>\n",
       "      <th>(g, s, h)</th>\n",
       "      <th>(o, g, b)</th>\n",
       "      <th>(z,  , y)</th>\n",
       "      <th>(k, y, t)</th>\n",
       "      <th>(e, e, g)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>81</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 7522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   (s, t)  (t, o)  (o, r)  (r, y)  (y,  )  ( , o)  (o, f)  (f,  )  ( , a)  \\\n",
       "0       7       5       6       2      10       9       5       5      12   \n",
       "1      25      19      47       5      33      65      25      29      81   \n",
       "\n",
       "   (a,  )  ...  (z, v, o)  (d, o, v)  (k, r, e)  (f, b, a)  (i, d, y)  \\\n",
       "0       9  ...          0          0          0          0          0   \n",
       "1      28  ...          0          0          0          0          0   \n",
       "\n",
       "   (g, s, h)  (o, g, b)  (z,  , y)  (k, y, t)  (e, e, g)  \n",
       "0          0          0          0          0          0  \n",
       "1          0          0          0          0          0  \n",
       "\n",
       "[2 rows x 7522 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([WordDictionaryCount_IMDB_Train[0],WordDictionaryCount_IMDB_Train[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai TF pada setiap data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFrequency(wordDict, ngram):\n",
    "    freqword = {}\n",
    "    ngramLength = len(ngram)\n",
    "    for word, count in wordDict.items():\n",
    "        freqword[word] = count\n",
    "    return freqword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequencyTermPerData(wordDictionary, ngram) :\n",
    "    CountFrequency = []\n",
    "    iterator = 0\n",
    "    while iterator < len(ngram):\n",
    "        CountFrequency.append(computeFrequency(wordDictionary[iterator], ngram[iterator]))\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    return CountFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountFrequency_Sentimen_Amazon_Train = getFrequencyTermPerData(WordDictionaryCount_Amazon_Train, ngram_amazon_train)\n",
    "CountFrequency_Sentimen_IMDB_Train = getFrequencyTermPerData(WordDictionaryCount_IMDB_Train, ngram_imdb_train)\n",
    "CountFrequency_Sentimen_Amazon_Test = getFrequencyTermPerData(WordDictionaryCount_Amazon_Test, ngram_amazon_test)\n",
    "CountFrequency_Sentimen_IMDB_Test = getFrequencyTermPerData(WordDictionaryCount_IMDB_Test, ngram_imdb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai IDF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(WordDict):\n",
    "    idfDict = {}\n",
    "    number_of_document_with_term_t_in_it = {}\n",
    "    N = len(WordDict)\n",
    "    \n",
    "    idfDict = dict.fromkeys(WordDict[0].keys(), 0)\n",
    "    for doc in WordDict:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        number_of_document_with_term_t_in_it[word] = float(val)\n",
    "        idfDict[word] = np.log((1+N) / (1+val)) + 1\n",
    "        \n",
    "    return idfDict, number_of_document_with_term_t_in_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_Amazon, number_of_document_with_term_t_Amazon_Train = computeIDF(WordDictionaryCount_Amazon_Train)\n",
    "IDF_IMDB, number_of_document_with_term_t_in_IMDB_Train = computeIDF(WordDictionaryCount_IMDB_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDFnonScale(CountFrequency, idfs):\n",
    "    tfidf = None\n",
    "    tfidf = {}\n",
    "    for word, val in CountFrequency.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfidf_nonscaled) :\n",
    "    tfidf_nonscaled = np.array(tfidf_nonscaled)\n",
    "    tfidf_list = tfidf_nonscaled/sum(tfidf_nonscaled**2)**0.5\n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(wordDictionary, CountFrequency, IDF):\n",
    "    TFIDF_non_scaled = None\n",
    "    TFIDF_non_scaled = []\n",
    "    iterator = 0\n",
    "    while iterator < len(wordDictionary):\n",
    "        TFIDF_non_scaled.append(computeTFIDFnonScale(CountFrequency[iterator], IDF))\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    TFIDF = []\n",
    "    iterator = 0\n",
    "    while iterator < len(wordDictionary):\n",
    "        tf_idf_list = list(TFIDF_non_scaled[iterator].values())\n",
    "        TFIDF.append(computeTFIDF(tf_idf_list))\n",
    "        iterator = iterator + 1\n",
    "        \n",
    "    return TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Amazon_Train = getTFIDF(WordDictionaryCount_Amazon_Train, CountFrequency_Sentimen_Amazon_Train, IDF_Amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_Amazon_Test = getTFIDF(WordDictionaryCount_Amazon_Test, CountFrequency_Sentimen_Amazon_Test, IDF_Amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_IMDB_Train = getTFIDF(WordDictionaryCount_IMDB_Train, CountFrequency_Sentimen_IMDB_Train, IDF_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_IMDB_Test = getTFIDF(WordDictionaryCount_IMDB_Test, CountFrequency_Sentimen_IMDB_Test, IDF_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toVectorizeList(TFIDF):\n",
    "    iterator  = 0\n",
    "    Vectorized = []\n",
    "    while iterator < len(TFIDF):\n",
    "        Vectorized.append(list(TFIDF[iterator]))\n",
    "        iterator = iterator + 1\n",
    "    return Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Amazon_Train = toVectorizeList(TFIDF_Amazon_Train)\n",
    "Vectorizer_Amazon_Test = toVectorizeList(TFIDF_Amazon_Test)\n",
    "Vectorizer_IMDB_Train = toVectorizeList(TFIDF_IMDB_Train)\n",
    "Vectorizer_IMDB_Test = toVectorizeList(TFIDF_IMDB_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akurasi dan Waktu Train Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waktu Training Amazon: 25.475s\n"
     ]
    }
   ],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_Amazon = RandomForestClassifier(\n",
    "        max_depth= 5, n_estimators = 800, random_state=42,\n",
    "        bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Amazon.fit(Vectorizer_Amazon_Train, Label_Amazon_Train)\n",
    "print(f\"\\nWaktu Training Amazon: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waktu prediksi (train): 2.629s\n",
      "waktu prediksi (test): 0.885s\n",
      "\n",
      "Skor Random Forest Train Amazon : 0.8978666666666667\n",
      "Skor Random Forest Test Amazon : 0.8184\n"
     ]
    }
   ],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_Amazon = RF_Classifier_Amazon.score(Vectorizer_Amazon_Train, Label_Amazon_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_Amazon = RF_Classifier_Amazon.score(Vectorizer_Amazon_Test, Label_Amazon_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train Amazon : {}\".format(Skor_Train_Amazon))\n",
    "print(\"Skor Random Forest Test Amazon : {}\".format(Skor_Test_Amazon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waktu Training IMDB: 7.578s\n"
     ]
    }
   ],
   "source": [
    "Waktu_Training = time()\n",
    "RF_Classifier_IMDB = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_IMDB.fit(Vectorizer_IMDB_Train, Label_IMDB_Train)\n",
    "print(f\"\\nWaktu Training IMDB: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waktu prediksi (train): 0.655s\n",
      "waktu prediksi (test): 0.801s\n",
      "\n",
      "Skor Random Forest Train : 0.992\n",
      "Skor Random Forest Test  : 0.7392\n",
      "----------------------------------------------------\n",
      "\n",
      "\n",
      "[[498   2]\n",
      " [  6 494]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       500\n",
      "           1       1.00      0.99      0.99       500\n",
      "\n",
      "    accuracy                           0.99      1000\n",
      "   macro avg       0.99      0.99      0.99      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "\n",
      "[[455 170]\n",
      " [156 469]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.73      0.74       625\n",
      "           1       0.73      0.75      0.74       625\n",
      "\n",
      "    accuracy                           0.74      1250\n",
      "   macro avg       0.74      0.74      0.74      1250\n",
      "weighted avg       0.74      0.74      0.74      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_IMDB = RF_Classifier_IMDB.score(Vectorizer_IMDB_Train, Label_IMDB_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_IMDB = RF_Classifier_IMDB.score(Vectorizer_IMDB_Test, Label_IMDB_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train : {}\".format(Skor_Train_IMDB))\n",
    "print(\"Skor Random Forest Test  : {}\".format(Skor_Test_IMDB))\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "\n",
    "RFClassifier_predict = RF_Classifier_IMDB.predict(Vectorizer_IMDB_Train)\n",
    "Confusion_matrix = confusion_matrix(Label_IMDB_Train, RFClassifier_predict)\n",
    "print(Confusion_matrix)\n",
    "print(metrics.classification_report(Label_IMDB_Train, RFClassifier_predict))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFClassifier_predict = RF_Classifier_IMDB.predict(Vectorizer_IMDB_Test)\n",
    "Confusion_matrix = confusion_matrix(Label_IMDB_Test, RFClassifier_predict)\n",
    "print(Confusion_matrix)\n",
    "print(metrics.classification_report(Label_IMDB_Test, RFClassifier_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mencari Term-Term yang interseksi antara Amazon dan IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Intersection = list(set(\n",
    "                            Vocabulary_IMDB_Train) & set (\n",
    "                            Vocabulary_Amazon_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6634\n"
     ]
    }
   ],
   "source": [
    "print(len(Vocabulary_Intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecomputeIDF(WordDictAmazon, WordDictYelp, additional_term_value, Vocabulary_Intersection):\n",
    "    idfDict = {}\n",
    "    number_of_document_with_term_t_in_it = {}\n",
    "    N = len(WordDictYelp) + len(WordDictAmazon)\n",
    "    idfDict = dict.fromkeys(WordDictYelp[0].keys(), 0)\n",
    "    for doc in WordDictYelp:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        if word in Vocabulary_Intersection :\n",
    "            val = val + additional_term_value[word]\n",
    "        idfDict[word] = np.log((1+N) / (1+val)) + 1\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_IDF_IMDB = RecomputeIDF(WordDictionaryCount_Amazon_Train, \n",
    "                            WordDictionaryCount_IMDB_Train, \n",
    "                            number_of_document_with_term_t_Amazon_Train, \n",
    "                            Vocabulary_Intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_TFIDF_Train = getTFIDF(WordDictionaryCount_IMDB_Train, CountFrequency_Sentimen_IMDB_Train, New_IDF_IMDB)\n",
    "New_TFIDF_Test = getTFIDF(WordDictionaryCount_IMDB_Test, CountFrequency_Sentimen_IMDB_Test, New_IDF_IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_TF_Train = toVectorizeList(New_TFIDF_Train)\n",
    "Vectorizer_TF_Test = toVectorizeList(New_TFIDF_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_TFIDF_Train = Vectorizer_TF_Train\n",
    "New_TFIDF_Test = Vectorizer_TF_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Waktu Training: 7.751s\n"
     ]
    }
   ],
   "source": [
    "Waktu_Training = time()\n",
    "New_RF_Classifier_Yelp = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, \n",
    "                                        max_features = 'auto')\n",
    "New_RF_Classifier_Yelp.fit(New_TFIDF_Train, Label_IMDB_Train)\n",
    "print(f\"\\nWaktu Training: {round(time()-Waktu_Training, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waktu prediksi (train): 0.646s\n",
      "waktu prediksi (test): 0.804s\n",
      "\n",
      "Skor Random Forest Train : 0.992\n",
      "Skor Random Forest Test  : 0.7448\n",
      "----------------------------------------------------\n",
      "\n",
      "\n",
      "[[499   1]\n",
      " [  7 493]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       500\n",
      "           1       1.00      0.99      0.99       500\n",
      "\n",
      "    accuracy                           0.99      1000\n",
      "   macro avg       0.99      0.99      0.99      1000\n",
      "weighted avg       0.99      0.99      0.99      1000\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "\n",
      "[[460 165]\n",
      " [154 471]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74       625\n",
      "           1       0.74      0.75      0.75       625\n",
      "\n",
      "    accuracy                           0.74      1250\n",
      "   macro avg       0.74      0.74      0.74      1250\n",
      "weighted avg       0.74      0.74      0.74      1250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Waktu_Predict_Train = time()\n",
    "Skor_Train_IMDB = New_RF_Classifier_Yelp.score(New_TFIDF_Train, Label_IMDB_Train)\n",
    "print(f\"waktu prediksi (train): {round(time()-Waktu_Predict_Train, 3)}s\")\n",
    "\n",
    "Waktu_Predict_Test = time()\n",
    "Skor_Test_IMDB = New_RF_Classifier_Yelp.score(New_TFIDF_Test, Label_IMDB_Test)\n",
    "print(f\"waktu prediksi (test): {round(time()-Waktu_Predict_Test, 3)}s\")\n",
    "\n",
    "print(\"\\nSkor Random Forest Train : {}\".format(Skor_Train_IMDB))\n",
    "print(\"Skor Random Forest Test  : {}\".format(Skor_Test_IMDB))\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "\n",
    "RFClassifier_predict = New_RF_Classifier_Yelp.predict(New_TFIDF_Train)\n",
    "Confusion_matrix = confusion_matrix(Label_IMDB_Train, RFClassifier_predict)\n",
    "print(Confusion_matrix)\n",
    "print(metrics.classification_report(Label_IMDB_Train, RFClassifier_predict))\n",
    "\n",
    "print(\"----------------------------------------------------\\n\\n\")\n",
    "RFClassifier_predict = New_RF_Classifier_Yelp.predict(New_TFIDF_Test)\n",
    "Confusion_matrix = confusion_matrix(Label_IMDB_Test, RFClassifier_predict)\n",
    "print(Confusion_matrix)\n",
    "print(metrics.classification_report(Label_IMDB_Test, RFClassifier_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
