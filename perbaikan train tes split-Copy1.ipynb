{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKRIPSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import ngrams\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "np.random.seed(0)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "poster_Stemmer = nltk.PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baca csv\n",
    "data_train_amazon = pd.read_csv('Amazon_Train.csv')\n",
    "data_train_yelp = pd.read_csv('Yelp_Train.csv')\n",
    "data_test_amazon = pd.read_csv('Amazon_Test.csv')\n",
    "data_test_yelp = pd.read_csv('Yelp_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train_amazon.head())\n",
    "print(\"\\n\")\n",
    "print(data_train_yelp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test_amazon.head())\n",
    "print(\"\\n\")\n",
    "print(data_test_yelp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing ke lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_Preprocessing_Amazon_Train = data_train_amazon\n",
    "data_Lowercase_Amazon_Train = []\n",
    "\n",
    "data_Preprocessing_Yelp_Train = data_train_yelp\n",
    "data_Lowercase_Yelp_Train = []\n",
    "\n",
    "while iterator < len(data_train_amazon) :\n",
    "    data_Lowercase_Amazon_Train.append(data_train_amazon.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_train_yelp) :\n",
    "    data_Lowercase_Yelp_Train.append(data_train_yelp.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_Preprocessing_Amazon_Test = data_test_amazon\n",
    "data_Lowercase_Amazon_Test = []\n",
    "\n",
    "data_Preprocessing_Yelp_Test = data_test_yelp\n",
    "data_Lowercase_Yelp_Test = []\n",
    "\n",
    "while iterator < len(data_test_amazon) :\n",
    "    data_Lowercase_Amazon_Test.append(data_test_amazon.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_test_yelp) :\n",
    "    data_Lowercase_Yelp_Test.append(data_test_yelp.Sentimen[iterator].lower())\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['Lowercase'] = data_Lowercase_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['Lowercase'] = data_Lowercase_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['Lowercase'] = data_Lowercase_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['Lowercase'] = data_Lowercase_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemoveNumber_Amazon_Train = []\n",
    "\n",
    "data_RemoveNumber_Yelp_Train = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Train) :\n",
    "    data_RemoveNumber_Amazon_Train.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Amazon_Train.Lowercase[iterator]))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Train) :\n",
    "    data_RemoveNumber_Yelp_Train.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Yelp_Train.Lowercase[iterator]))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemoveNumber_Amazon_Test = []\n",
    "\n",
    "data_RemoveNumber_Yelp_Test = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Test) :\n",
    "    data_RemoveNumber_Amazon_Test.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Amazon_Test.Lowercase[iterator]))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Test) :\n",
    "    data_RemoveNumber_Yelp_Test.append(re.sub(r\"\\d+\", \"\",data_Preprocessing_Yelp_Test.Lowercase[iterator]))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['RemoveNumber'] = data_RemoveNumber_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['RemoveNumber'] = data_RemoveNumber_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['RemoveNumber'] = data_RemoveNumber_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['RemoveNumber'] = data_RemoveNumber_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan tanda baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemovePunctuation_Amazon_Train = []\n",
    "\n",
    "data_RemovePunctuation_Yelp_Train = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Train) :\n",
    "    data_RemovePunctuation_Amazon_Train.append(data_Preprocessing_Amazon_Train.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Train) :\n",
    "    data_RemovePunctuation_Yelp_Train.append(data_Preprocessing_Yelp_Train.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "data_RemovePunctuation_Amazon_Test = []\n",
    "\n",
    "data_RemovePunctuation_Yelp_Test = []\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Test) :\n",
    "    data_RemovePunctuation_Amazon_Test.append(data_Preprocessing_Amazon_Test.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Test) :\n",
    "    data_RemovePunctuation_Yelp_Test.append(data_Preprocessing_Yelp_Test.RemoveNumber[iterator].translate(str.maketrans('','', string.punctuation)))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['RemovePunctuation'] = data_RemovePunctuation_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['RemovePunctuation'] = data_RemovePunctuation_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['RemovePunctuation'] = data_RemovePunctuation_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['RemovePunctuation'] = data_RemovePunctuation_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghilangkan Non alfabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "\n",
    "data_Regex_alpabet_only_Amazon_Train = []\n",
    "\n",
    "data_Regex_alpabet_only_Yelp_Train = []\n",
    "\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Train) :\n",
    "    data_Regex_alpabet_only_Amazon_Train.append(\" \".join(re.findall(\"[a-zA-Z]+\", data_Preprocessing_Amazon_Train.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Train) :\n",
    "    data_Regex_alpabet_only_Yelp_Train.append(\" \".join(re.findall(r\"[a-zA-Z]+\", data_Preprocessing_Yelp_Train.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "\n",
    "data_Regex_alpabet_only_Amazon_Test = []\n",
    "\n",
    "data_Regex_alpabet_only_Yelp_Test = []\n",
    "\n",
    "\n",
    "while iterator < len(data_Preprocessing_Amazon_Test) :\n",
    "    data_Regex_alpabet_only_Amazon_Test.append(\" \".join(re.findall(\"[a-zA-Z]+\", data_Preprocessing_Amazon_Test.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1\n",
    "\n",
    "iterator = 0\n",
    "\n",
    "while iterator < len(data_Preprocessing_Yelp_Test) :\n",
    "    data_Regex_alpabet_only_Yelp_Test.append(\" \".join(re.findall(r\"[a-zA-Z]+\", data_Preprocessing_Yelp_Test.RemovePunctuation[iterator])))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Train['Regex'] = data_Regex_alpabet_only_Amazon_Train\n",
    "data_Preprocessing_Amazon_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Amazon_Test['Regex'] = data_Regex_alpabet_only_Amazon_Test\n",
    "data_Preprocessing_Amazon_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Train['Regex'] = data_Regex_alpabet_only_Yelp_Train\n",
    "data_Preprocessing_Yelp_Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Preprocessing_Yelp_Test['Regex'] = data_Regex_alpabet_only_Yelp_Test\n",
    "data_Preprocessing_Yelp_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clean_Amazon_Sentiment_Train = data_Preprocessing_Amazon_Train['Regex']\n",
    "Clean_Yelp_Sentiment_Train = data_Preprocessing_Yelp_Train['Regex']\n",
    "Label_Amazon_Train = data_Preprocessing_Amazon_Train['Label']\n",
    "Label_Yelp_Train = data_Preprocessing_Yelp_Train['Label']\n",
    "\n",
    "Clean_Amazon_Sentiment_Test = data_Preprocessing_Amazon_Test['Regex']\n",
    "Clean_Yelp_Sentiment_Test = data_Preprocessing_Yelp_Test['Regex']\n",
    "Label_Amazon_Test = data_Preprocessing_Amazon_Test['Label']\n",
    "Label_Yelp_Test = data_Preprocessing_Yelp_Test['Label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram (bigram dan trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "Vocabulary_Amazon_Train = []\n",
    "ngram_amazon_train = []\n",
    "\n",
    "while iterator < len(Clean_Amazon_Sentiment_Train) :\n",
    "    ngram = 2\n",
    "    sentence = Clean_Amazon_Sentiment_Train[iterator]\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = vocab\n",
    "\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Amazon_Train) :\n",
    "            Vocabulary_Amazon_Train.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "    \n",
    "    ngram = 3\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = temp + vocab\n",
    "    ngram_amazon_train.append(temp)\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Amazon_Train) :\n",
    "            Vocabulary_Amazon_Train.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "        \n",
    "    iterator = iterator + 1\n",
    "    vocab = None\n",
    "    del vocab\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "Vocabulary_Amazon_Test = []\n",
    "ngram_amazon_test = []\n",
    "\n",
    "while iterator < len(Clean_Amazon_Sentiment_Test) :\n",
    "    ngram = 2\n",
    "    sentence = Clean_Amazon_Sentiment_Test[iterator]\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = vocab\n",
    "\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Amazon_Test) :\n",
    "            Vocabulary_Amazon_Test.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "    \n",
    "    ngram = 3\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = temp + vocab\n",
    "    ngram_amazon_test.append(temp)\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Amazon_Test) :\n",
    "            Vocabulary_Amazon_Test.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "        \n",
    "    iterator = iterator + 1\n",
    "    vocab = None\n",
    "    del vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "Vocabulary_Yelp_Train = []\n",
    "ngram_yelp_train = []\n",
    "\n",
    "while iterator < len(Clean_Yelp_Sentiment_Train) :\n",
    "    ngram = 2\n",
    "    sentence = Clean_Yelp_Sentiment_Train[iterator]\n",
    "    \n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "  \n",
    "    temp = vocab\n",
    "\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Yelp_Train) :\n",
    "            Vocabulary_Yelp_Train.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "    \n",
    "    ngram = 3\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = temp + vocab\n",
    "    ngram_yelp_train.append(temp)\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Yelp_Train) :\n",
    "            Vocabulary_Yelp_Train.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "        \n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "Vocabulary_Yelp_Test = []\n",
    "ngram_yelp_test = []\n",
    "\n",
    "while iterator < len(Clean_Yelp_Sentiment_Test) :\n",
    "    ngram = 2\n",
    "    sentence = Clean_Yelp_Sentiment_Test[iterator]\n",
    "    \n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "  \n",
    "    temp = vocab\n",
    "\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Yelp_Test) :\n",
    "            Vocabulary_Yelp_Test.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "    \n",
    "    ngram = 3\n",
    "    vocab = [sentence[i:i+ngram] for i in range(len(sentence)-ngram+1)]\n",
    "    temp = temp + vocab\n",
    "    ngram_yelp_test.append(temp)\n",
    "    iterator2 = 0\n",
    "    while iterator2 < len (vocab):\n",
    "        if(vocab[iterator2] not in Vocabulary_Yelp_Test) :\n",
    "            Vocabulary_Yelp_Test.append(vocab[iterator2])\n",
    "        iterator2 = iterator2 + 1\n",
    "        \n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me inisialisasi jumlah frekuensi Gram di Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Amazon_Train = []\n",
    "WordDictionaryCount_Yelp_Train = []\n",
    "\n",
    "iterator = 0\n",
    "while iterator < len(Clean_Amazon_Sentiment_Train):\n",
    "    #Membuat worddictionary gabungan antara vocab amazon test dan train\n",
    "    WordDictionaryCount_Amazon_Train.append(dict.fromkeys(Vocabulary_Amazon_Train + Vocabulary_Amazon_Test , 0))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(Clean_Yelp_Sentiment_Train):\n",
    "    WordDictionaryCount_Yelp_Train.append(dict.fromkeys(Vocabulary_Yelp_Train + Vocabulary_Yelp_Test, 0))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordDictionaryCount_Amazon_Test = []\n",
    "WordDictionaryCount_Yelp_Test = []\n",
    "\n",
    "iterator = 0\n",
    "while iterator < len(Clean_Amazon_Sentiment_Test):\n",
    "    WordDictionaryCount_Amazon_Test.append(dict.fromkeys(Vocabulary_Amazon_Test + Vocabulary_Amazon_Train, 0))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(Clean_Yelp_Sentiment_Test):\n",
    "    WordDictionaryCount_Yelp_Test.append(dict.fromkeys(Vocabulary_Yelp_Test + Vocabulary_Yelp_Train, 0))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "while iterator < len(Clean_Amazon_Sentiment_Train) :\n",
    "    for gram in ngram_amazon_train[iterator] :\n",
    "        WordDictionaryCount_Amazon_Train[iterator][gram] += 1\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(Clean_Yelp_Sentiment_Train) :\n",
    "    for gram in ngram_yelp_train[iterator] :\n",
    "        WordDictionaryCount_Yelp_Train[iterator][gram] += 1\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "while iterator < len(Clean_Amazon_Sentiment_Test) :\n",
    "    for gram in ngram_amazon_test[iterator] :\n",
    "        WordDictionaryCount_Amazon_Test[iterator][gram] += 1\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(Clean_Yelp_Sentiment_Test) :\n",
    "    for gram in ngram_yelp_test[iterator] :\n",
    "        WordDictionaryCount_Yelp_Test[iterator][gram] += 1\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Amazon_Train[0],WordDictionaryCount_Amazon_Train[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Amazon_Test[0],WordDictionaryCount_Amazon_Test[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Yelp_Train[0],WordDictionaryCount_Yelp_Train[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([WordDictionaryCount_Yelp_Test[0],WordDictionaryCount_Yelp_Test[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai TF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, ngram):\n",
    "    tfDict = {}\n",
    "    ngramLength = len(ngram)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(ngramLength)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(WordDictionaryCount_Amazon_Train))\n",
    "print(len(WordDictionaryCount_Yelp_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf_Sentimen_Amazon_Train = []\n",
    "Tf_Sentimen_Yelp_Train = []\n",
    "Tf_Sentimen_Amazon_Test = []\n",
    "Tf_Sentimen_Yelp_Test = []\n",
    "\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Amazon_Train):\n",
    "    Tf_Sentimen_Amazon_Train.append(computeTF(WordDictionaryCount_Amazon_Train[iterator], ngram_amazon_train[iterator]))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Amazon_Test):\n",
    "    Tf_Sentimen_Amazon_Test.append(computeTF(WordDictionaryCount_Amazon_Test[iterator], ngram_amazon_test[iterator]))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp_Train):\n",
    "    Tf_Sentimen_Yelp_Train.append(computeTF(WordDictionaryCount_Yelp_Train[iterator], ngram_yelp_train[iterator]))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp_Test):\n",
    "    Tf_Sentimen_Yelp_Test.append(computeTF(WordDictionaryCount_Yelp_Test[iterator], ngram_yelp_test[iterator]))\n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai IDF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def computeIDF(WordDict):\n",
    "    idfDict = {}\n",
    "    number_of_document_with_term_t_in_it = {}\n",
    "    N = len(WordDict)\n",
    "    \n",
    "    idfDict = dict.fromkeys(WordDict[0].keys(), 0)\n",
    "    for doc in WordDict:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        number_of_document_with_term_t_in_it[word] = float(val)\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "        \n",
    "    return idfDict, number_of_document_with_term_t_in_it\n",
    "\n",
    "IDF_Amazon_Train, number_of_document_with_term_t_Amazon_Train = computeIDF(WordDictionaryCount_Amazon_Train)\n",
    "IDF_Yelp_Train, number_of_document_with_term_t_in_Yelp_Train = computeIDF(WordDictionaryCount_Yelp_Train)\n",
    "\n",
    "IDF_Amazon_Test, number_of_document_with_term_t_Amazon_Test = computeIDF(WordDictionaryCount_Amazon_Test)\n",
    "IDF_Yelp_Test, number_of_document_with_term_t_in_Yelp_Test = computeIDF(WordDictionaryCount_Yelp_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung nilai TF-IDF pada setiap data Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(Tf_Sentimen_Amazon, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in Tf_Sentimen_Amazon.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "TF_IDF_Amazon_Train = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Amazon_Train):\n",
    "    TF_IDF_Amazon_Train.append(computeTFIDF(Tf_Sentimen_Amazon_Train[iterator], IDF_Amazon_Train))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "TF_IDF_Amazon_Test = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Amazon_Test):\n",
    "    TF_IDF_Amazon_Test.append(computeTFIDF(Tf_Sentimen_Amazon_Test[iterator], IDF_Amazon_Test))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "TF_IDF_Yelp_Train = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp_Train):\n",
    "    TF_IDF_Yelp_Train.append(computeTFIDF(Tf_Sentimen_Yelp_Train[iterator], IDF_Yelp_Train))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "TF_IDF_Yelp_Test = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp_Test):\n",
    "    TF_IDF_Yelp_Test.append(computeTFIDF(Tf_Sentimen_Yelp_Test[iterator], IDF_Yelp_Test))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menampung nilai TF-IDF pada variabel Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Amazon_Train = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Amazon_Train):\n",
    "    Vectorizer_Amazon_Train.append(list(TF_IDF_Amazon_Train[iterator].values()))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "Vectorizer_Yelp_Train = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Yelp_Train):\n",
    "    Vectorizer_Yelp_Train.append(list(TF_IDF_Yelp_Train[iterator].values()))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "Vectorizer_Amazon_Test = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Amazon_Test):\n",
    "    Vectorizer_Amazon_Test.append(list(TF_IDF_Amazon_Test[iterator].values()))\n",
    "    iterator = iterator + 1\n",
    "    \n",
    "Vectorizer_Yelp_Test = []\n",
    "iterator  = 0\n",
    "while iterator < len(TF_IDF_Yelp_Test):\n",
    "    Vectorizer_Yelp_Test.append(list(TF_IDF_Yelp_Test[iterator].values()))\n",
    "    iterator = iterator + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frame Vectorizer Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Amazon_Train = pd.DataFrame(Vectorizer_Amazon_Train, columns = Vocabulary_Amazon_Train)\n",
    "print (DataFrame_Amazon_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Vectorizer Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame_Yelp_Train = pd.DataFrame(Vectorizer_Yelp_Train, columns = Vocabulary_Yelp_Train)\n",
    "print (DataFrame_Yelp_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jumlah data label positif dan negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data positif negatif amazon\")\n",
    "DataFrame_Label = pd.DataFrame(Label_Amazon_Train)\n",
    "print(DataFrame_Label['Label'].value_counts())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Data positif negatif yelp\")\n",
    "DataFrame_Label = pd.DataFrame(Label_Yelp_Train)\n",
    "print(DataFrame_Label['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Amazon data\")\n",
    "Vectorizer_Amazon_Train = np.array(Vectorizer_Amazon_Train)\n",
    "Vectorizer_Yelp_Train = np.array(Vectorizer_Yelp_Train)\n",
    "Vectorizer_Amazon_Test = np.array(Vectorizer_Amazon_Test)\n",
    "Vectorizer_Yelp_Test = np.array(Vectorizer_Yelp_Test)\n",
    "\n",
    "Label_Amazon_Train = np.array(Label_Amazon_Train)\n",
    "Label_Yelp_Train = np.array(Label_Yelp_Train)\n",
    "Label_Amazon_Test = np.array(Label_Amazon_Test)\n",
    "Label_Yelp_Test = np.array(Label_Yelp_Test)\n",
    "\n",
    "print(\"data train shape : {}\".format(Vectorizer_Amazon_Train.shape))\n",
    "print(\"data test shape : {}\".format(Vectorizer_Amazon_Test.shape))\n",
    "print(\"label train shape : {}\".format(Label_Amazon_Train.shape))\n",
    "print(\"label test shape : {}\".format(Label_Amazon_Test.shape))\n",
    "\n",
    "print(\"\\nYelp data\")\n",
    "print(\"data train shape : {}\".format(Vectorizer_Yelp_Train.shape))\n",
    "print(\"data test shape : {}\".format(Vectorizer_Yelp_Test.shape))\n",
    "print(\"label train shape : {}\".format(Label_Yelp_Train.shape))\n",
    "print(\"label test shape : {}\".format(Label_Yelp_Test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# dr = PCA(n_components = 300)\n",
    "# dr.fit(data_train_amazon)\n",
    "# data_train_amazon = dr.transform(data_train_amazon)\n",
    "# data_test_amazon = dr.transform(data_test_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skor Akurasi Amazon dan Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_Classifier_Amazon = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "RF_Classifier_Amazon.fit(Vectorizer_Amazon_Train, Label_Amazon_Train)\n",
    "print(\"skor RF_Amazon: {}\".format(RF_Classifier_Amazon.score(Vectorizer_Amazon_Test, Label_Amazon_Test)))\n",
    "\n",
    "# RF_Classifier_Yelp = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "#                                        bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "# RF_Classifier_Yelp.fit(data_train_yelp, label_train_yelp)\n",
    "# print(\"skor RF_Yelp: {}\".format(RF_Classifier_Yelp.score(data_test_yelp, label_test_yelp)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_document_with_term_t_Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary_Intersection = []\n",
    "iterator = 0\n",
    "while iterator < len(Vocabulary_Yelp) :\n",
    "    if Vocabulary_Yelp[iterator] in Vocabulary_Amazon :\n",
    "        Vocabulary_Intersection.append(Vocabulary_Yelp[iterator])\n",
    "    iterator = iterator + 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecomputeIDF(WordDictAmazon, WordDict, additional_term_value, Vocabulary_Intersection):\n",
    "    idfDict = {}\n",
    "    number_of_document_with_term_t_in_it = {}\n",
    "    N = len(WordDict) + len(WordDictAmazon)\n",
    "    print(len(WordDict))\n",
    "    print(len(WordDictAmazon))\n",
    "    idfDict = dict.fromkeys(WordDict[0].keys(), 0)\n",
    "    for doc in WordDict:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        if word in Vocabulary_Intersection :\n",
    "            print(additional_term_value[word])\n",
    "            val = val + additional_term_value[word]\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "\n",
    "New_IDF_Yelp = RecomputeIDF(WordDictionaryCount_Amazon,WordDictionaryCount_Yelp, number_of_document_with_term_t_Amazon, Vocabulary_Intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_TF_IDF_Yelp = []\n",
    "iterator = 0\n",
    "while iterator < len(WordDictionaryCount_Yelp):\n",
    "    New_TF_IDF_Yelp.append(computeTFIDF(Tf_Sentimen_Yelp[iterator], New_IDF_Yelp))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_Vectorizer_Yelp = []\n",
    "iterator  = 0\n",
    "while iterator < len(New_TF_IDF_Yelp):\n",
    "    New_Vectorizer_Yelp.append(list(New_TF_IDF_Yelp[iterator].values()))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_train_yelp, new_data_test_yelp, new_label_train_yelp, new_label_test_yelp = train_test_split((New_Vectorizer_Yelp), Label_Yelp, random_state = 0, test_size = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_RF_Classifier_Yelp = RandomForestClassifier(max_depth= 5, n_estimators = 800, random_state=42,\n",
    "                                       bootstrap = False, min_samples_split = 5, min_samples_leaf = 1, max_features = 'auto')\n",
    "New_RF_Classifier_Yelp.fit(new_data_train_yelp, new_label_train_yelp)\n",
    "print(\"skor RF_Yelp: {}\".format(New_RF_Classifier_Yelp.score(new_data_test_yelp, new_label_test_yelp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
